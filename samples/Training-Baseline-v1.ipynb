{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf1\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import quantutils.dataset.pipeline as ppl\n",
    "import quantutils.dataset.ml as mlutils\n",
    "from marketinsights.api.model import MarketInsightsModel\n",
    "from marketinsights.remote.ml import MIAssembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ID1 = \"4234f0f1b6fcc17f6458696a6cdf5101\"  # DOW\n",
    "\n",
    "assembly = MIAssembly(secret=\"marketinsights-k8s-cred\")\n",
    "\n",
    "# Training Set\n",
    "dataset, descriptor = assembly.get_dataset_by_id(DATASET_ID1, debug=False)\n",
    "train_x, train_y = ppl.splitCol(dataset, NUM_FEATURES)\n",
    "train_x = tf.cast(train_x, tf.float32)\n",
    "train_y = tf.cast(train_y, tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEATURES = (2 * 4) + 1\n",
    "NUM_LABELS = 1\n",
    "\n",
    "HIDDEN_UNITS = 32\n",
    "# The random seed that defines initialization.\n",
    "SEED = 42\n",
    "# The stdev of the initialised random weights\n",
    "STDEV = 0.1\n",
    "# Network bias\n",
    "BIAS = 0.1\n",
    "\n",
    "class MLModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    # Initialize model parameters\n",
    "    self.Theta1 = tf.Variable(tf.random.normal([HIDDEN_UNITS, NUM_FEATURES], stddev=STDEV, seed=SEED))\n",
    "    self.Theta2 = tf.Variable(tf.random.normal([NUM_LABELS, HIDDEN_UNITS], stddev=STDEV, seed=SEED))\n",
    "    self.bias = tf.Variable(tf.constant(BIAS, shape=[NUM_LABELS]))\n",
    "    self.lam = tf.constant(0.001, tf.float32)\n",
    "\n",
    "  @tf.function\n",
    "  def call(self, x):\n",
    "    layer1 = tf.nn.sigmoid(tf.matmul(x, tf.transpose(self.Theta1)))\n",
    "    output = tf.nn.bias_add(tf.matmul(layer1, tf.transpose(self.Theta2)), self.bias)\n",
    "    return output\n",
    "\n",
    "  def loss(self, y_pred, y):\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=y_pred))\n",
    "    \n",
    "    # Regularization using L2 Loss function\n",
    "    regularizer = tf.nn.l2_loss(self.Theta1) + tf.nn.l2_loss(self.Theta2)\n",
    "    reg = (self.lam / tf.cast(tf.shape(y)[0], tf.float32)) * regularizer\n",
    "    loss_reg = loss + reg\n",
    "    \n",
    "    return loss_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error for step 0: 1.419\n",
      "Mean squared error for step 100: 0.693\n",
      "Mean squared error for step 200: 0.692\n",
      "Mean squared error for step 300: 0.692\n",
      "Mean squared error for step 400: 0.692\n",
      "Mean squared error for step 500: 0.692\n",
      "Mean squared error for step 600: 0.692\n",
      "Mean squared error for step 700: 0.691\n",
      "Mean squared error for step 800: 0.691\n",
      "Mean squared error for step 900: 0.691\n"
     ]
    }
   ],
   "source": [
    "# Set training parameters\n",
    "epochs = 1000\n",
    "learning_rate = 3.0\n",
    "losses = []\n",
    "\n",
    "mlmodel = MLModel()\n",
    "\n",
    "# Format training loop\n",
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "      batch_loss = mlmodel.loss(mlmodel(train_x), train_y)\n",
    "    # Update parameters with respect to the gradient calculations\n",
    "    grads = tape.gradient(batch_loss, mlmodel.variables)\n",
    "    for g,v in zip(grads, mlmodel.variables):\n",
    "        v.assign_sub(learning_rate*g)\n",
    "        \n",
    "    # Keep track of model loss per epoch\n",
    "    loss = mlmodel.loss(mlmodel(train_x), train_y)\n",
    "    losses.append(loss)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Mean squared error for step {epoch}: {loss.numpy():0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Won : 699.0\n",
      "Lost : 678.0\n",
      "Total : 1377.0\n",
      "Diff : 21.0\n",
      "Edge : 1.5250544662309369%\n",
      "Information Coefficient : 0.015250563621520996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5076253"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlutils.evaluate(ppl.onehot(tf.nn.sigmoid(mlmodel(train_x)).numpy()), ppl.onehot(train_y.numpy()), threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev3.9",
   "language": "python",
   "name": "dev3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
