{
    "nbformat_minor": 2, 
    "cells": [
        {
            "source": "## Model : marketdirection\n### Description :\nThis model uses a Tensorflow neural network to predict the direction of a market in the next Y periods, based on the values of the previous X periods. \n\n### Model Attributes :\n- FFNN\n- Boosting\n- Re-training of entire network for each additional period\n\n### USP :\n- Normalised market data (between 0 and 1) to highlight common patterns at any time scale.\n- Utilises similar markets to increase size of training set\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "MODEL_ID = \"3a491b1a-8af6-416d-aa14-f812cbd660bb\"\n\nMARKET1 = \"DOW\"\nMARKET2 = \"SPY\"\n\nPIPELINE_ID = \"marketdirection\""
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Collecting git+https://github.com/cwilko/quantutils.git\n  Cloning https://github.com/cwilko/quantutils.git to /gpfs/global_fs01/sym_shared/YPProdSpark/user/sc1c-81b7dbb381fb6a-c4b9ad2fa578/notebook/tmp/pip-S6Z3sr-build\nInstalling collected packages: quantutils\n  Found existing installation: quantutils 1.0.0\n    Uninstalling quantutils-1.0.0:\n      Successfully uninstalled quantutils-1.0.0\n  Running setup.py install for quantutils ... \u001b[?25ldone\n\u001b[?25hSuccessfully installed quantutils-1.0.0\n"
                }
            ], 
            "source": "#\n# Get dataset from MI API #\n#\n\nimport pandas\nimport sys\nimport gc\n\n!pip install --upgrade git+https://github.com/cwilko/quantutils.git\nimport quantutils.dataset.pipeline as ppl\nfrom quantutils.api.bluemix import ObjectStore, Metrics\nfrom quantutils.api.marketinsights import MarketInsights\n\nmetrics = Metrics('cred/metrics_cred.json')\nmi = MarketInsights('cred/MIOapi_cred.json')\nobjStore = ObjectStore('cred/object_storage_cred.json')\n\nCONFIG = mi.get_model(MODEL_ID)\n\nmkt1 = mi.get_dataset(MARKET1, PIPELINE_ID)\nmkt2 = mi.get_dataset(MARKET2, PIPELINE_ID)\n\n# Interleave (part of the \"added insight\" for this model)\nmkt1, mkt2, isect = ppl.intersect(mkt1,mkt2)\ndataset = ppl.interleave(mkt1,mkt2)"
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "testSetLength = 430\ntraining_set = dataset[:-(testSetLength)]\ntest_set = dataset[-(testSetLength):]"
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import numpy as np\nimport os\n\n# TODO : Pull out of pipeline config?\n##### Specific to the data ##\nNUM_FEATURES = (2 * 4) + 1\nNUM_LABELS = 2\n#############################"
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "ename": "ImportError", 
                    "evalue": "No module named quanutils.model.ml", 
                    "traceback": [
                        "\u001b[0;31m\u001b[0m", 
                        "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-8-5d38f4016c9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mquanutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquantutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmlutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mImportError\u001b[0m: No module named quanutils.model.ml"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "from quanutils.model.ml import Model\nimport quantutils.mode.utils as mlutils\n\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "##\n## BOOTSTRAP/BOOSTING TRAINING WITH LOO\n##\n\nprint \"Training\",\npredictions = np.array([]).reshape(0,2)\nbstrapTrainingSet = training_set\nthreshold = .0\n_, test_y = ppl.splitCol(test_set, NUM_FEATURES)\ninitialTestValue = 0\n\nbstrapTrainingSet = bstrapTrainingSet.append(pandas.DataFrame(test_set.values[:initialTestValue,:]))\n\ntry:\n    for i in range(initialTestValue,len(test_set),2):\n\n        test_rows = pandas.DataFrame(test_set.values[[i, i+1],:])\n        success = False\n        retry = 0\n        while ((~success) & (retry<TRN_CNF['training_retries'])):\n            try:\n                ## CHOOSE BOOTSTRAP OR BOOST\n                results = boostingTrain(bstrapTrainingSet, test_rows, TRN_CNF['lamda'], TRN_CNF['iterations'], CONFIG['debug'])\n                #results = bootstrapTrain(bstrapTrainingSet, test_rows, TRN_CNF['lamda'], TRN_CNF['iterations'], CONFIG['debug'])\n                predictions =  np.concatenate([predictions, np.nanmean(results[\"test_predictions\"], axis=0)])    \n                success = True\n            except ValueError: \n                print \"Value error\"\n                #log.emit_log( {'app_name': 'Experiment2','type': 'error','message': \"ValueError - Retrying...\"})\n                retry = retry + 1\n        \n        if (~success):\n            # TODO : Log this\n            print \"Failed to train after several retries\"\n            break\n            \n        bstrapTrainingSet = bstrapTrainingSet.append(test_rows)\n        \n        if (TRN_CNF['fixed_training_set_size']):\n            # Window\n            bstrapTrainingSet = bstrapTrainingSet[-len(training_set):]\n\n        res = evaluate(predictions, test_y[initialTestValue:initialTestValue+len(predictions),:], threshold)\n        msg = str(\"Results after %d iterations, %.2f precision, %.2f recall at %.2f threshold\" % (i+2, res[0], res[1], threshold))\n        print \".\"\n        print msg\n\n        #log.emit_log( {'app_name': 'Experiment2','type': 'result','message': msg})\n        metrics.send([{'name':'MI.precision', 'value':res[0].tolist()},{'name':'MI.recall', 'value':res[1].tolist()}])\n\n        pandas.DataFrame(predictions).to_csv(\"results.csv\", header=False, index=False)\n        objStore.put_file('Experiment2', \"results2.csv\", \"results2.csv\")\n\n        # Try to free memory\n        gc.collect()\nexcept:\n    print(\"Unexpected error: %s\" % sys.exc_info()[0])\n    #log.emit_log( {'app_name': 'Experiment2','type': 'error','message': str(\"Unexpected error: %s\" % sys.exc_info()[0])})\n    raise\n    "
        }, 
        {
            "execution_count": 250, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Training . . . Iterations : 1 Lambda : 0.00, Threshold : 0.00\nTraining loss : 0.21+/-0.00, precision : 0.91+/-0.00, recall : 1.00+/-0.00, F : 0.96+/-0.00\nValidation loss : 2.85+/-0.00, precision : 0.58+/-0.00, recall : 1.00+/-0.00, F : 0.73+/-0.00\nTest loss : 3.25+/-0.00, precision : 0.56+/-0.00, recall : 1.00+/-0.00, F : 0.72+/-0.00\nIteration : 0 Lambda : 0.00, Threshold : 0.00\nTraining loss : 0.21+/-0.00, precision : 0.91+/-0.00, recall : 1.00+/-0.00, F : 0.96+/-0.00\nValidation loss : 2.85+/-0.00, precision : 0.58+/-0.00, recall : 1.00+/-0.00, F : 0.73+/-0.00\nTest loss : 3.25+/-0.00, precision : 0.56+/-0.00, recall : 1.00+/-0.00, F : 0.72+/-0.00\n"
                }, 
                {
                    "execution_count": 250, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "(0.5604651, 1.0, 0.71833083399477315)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "##\n## BOOTSTRAP TRAINING\n##\n\nprint \"Training\",\n_, test_y = ppl.splitCol(test_set, NUM_FEATURES)\nresults = bootstrapTrain(training_set, test_set, TRN_CNF['lamda'], TRN_CNF['iterations'], TRN_CNF['threshold'], True)\npredictions2 =  np.nanmean(results[\"test_predictions\"], axis=0)\nevaluate(predictions2, test_y, .0)"
        }, 
        {
            "execution_count": 258, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Training . . . Iterations : 1 Lambda : 0.00, Threshold : 0.00\nTraining loss : 0.35+/-0.00, precision : 0.85+/-0.00, recall : 1.00+/-0.00, F : 0.92+/-0.00\nValidation loss : 1.45+/-0.00, precision : 0.56+/-0.00, recall : 1.00+/-0.00, F : 0.72+/-0.00\nTest loss : 1.70+/-0.00, precision : 0.52+/-0.00, recall : 1.00+/-0.00, F : 0.68+/-0.00\nIteration : 0 Lambda : 0.00, Threshold : 0.00\nTraining loss : 0.35+/-0.00, precision : 0.85+/-0.00, recall : 1.00+/-0.00, F : 0.92+/-0.00\nValidation loss : 1.45+/-0.00, precision : 0.56+/-0.00, recall : 1.00+/-0.00, F : 0.72+/-0.00\nTest loss : 1.70+/-0.00, precision : 0.52+/-0.00, recall : 1.00+/-0.00, F : 0.68+/-0.00\n"
                }, 
                {
                    "execution_count": 258, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "(0.51860464, 1.0, 0.68300151841054135)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "##\n## BOOTSTRAP TRAINING\n##\n\nprint \"Training\",\n_, test_y = ppl.splitCol(test_set, NUM_FEATURES)\nresults = bootstrapTrain(training_set, test_set, TRN_CNF['lamda'], TRN_CNF['iterations'], TRN_CNF['threshold'], True)\npredictions2 =  np.nanmean(results[\"test_predictions\"], axis=0)\nevaluate(predictions2, test_y, .0)"
        }, 
        {
            "execution_count": 194, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "t1 = Theta1.eval()\nt2 = Theta2.eval()"
        }, 
        {
            "execution_count": 210, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 210, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[0]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "range(0, 1)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 2.1", 
            "name": "python2-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "2.7.11", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython2", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}