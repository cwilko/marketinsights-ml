{
    "nbformat_minor": 2, 
    "cells": [
        {
            "source": "## Model : marketdirection\n### Description :\nThis model uses a Tensorflow neural network to predict the direction of a market in the next Y periods, based on the values of the previous X periods. \n\n### Model Attributes :\n- FFNN\n- Boosting\n- Re-training of entire network for each additional period\n\n### USP :\n- Normalised market data (between 0 and 1) to highlight common patterns at any time scale.\n- Utilises similar markets to increase size of training set\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "MODEL_ID = \"fdbe5895-0327-49d9-83e9-2246dbe1858b\"\n\nMARKET1 = \"DOW\"\nMARKET2 = \"SPY\"\n\nPIPELINE_ID = \"marketdirection\"\n\n# TODO : Pull out of pipeline config?\n##### Specific to the data ##\nNUM_FEATURES = (2 * 4) + 1\nNUM_LABELS = 2\n#############################", 
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "#\n# Get dataset from MI API #\n#\n\nimport pandas\nimport sys\nimport gc\nimport uuid\nimport numpy as np\n\n#!pip install --upgrade git+https://github.com/cwilko/quantutils.git\n\nimport quantutils.dataset.pipeline as ppl\nfrom quantutils.api.bluemix import ObjectStore, Metrics, Logger\nfrom quantutils.api.marketinsights import MarketInsights\nimport quantutils.model.utils as mlutils\nfrom quantutils.model.ml import Model\n\nmetrics = Metrics('cred/metrics_cred.json')\nmi = MarketInsights('cred/MIOapi_cred.json')\nobjStore = ObjectStore('cred/object_storage_cred.json')\nlog = Logger('MarketInsights-ML','cred/logging_cred.json')\n\n# Logging helper function\ntag = lambda x,y : \"\".join([\"(\", x, \":\", str(y+1), \") \"])\n\nCONFIG = mi.get_model(MODEL_ID)\nTRN_CNF = CONFIG['training']\n\nmkt1 = mi.get_dataset(MARKET1, PIPELINE_ID)\nmkt2 = mi.get_dataset(MARKET2, PIPELINE_ID)\n\n# Interleave (part of the \"added insight\" for this model)\nmkt1, mkt2, isect = ppl.intersect(mkt1,mkt2)\ndataset = ppl.interleave(mkt1,mkt2)\n\n# TODO : This should be config, probably as a proportion\nTEST_SET_SIZE = 430\nTRAINING_SET_SIZE = len(dataset) - TEST_SET_SIZE\nWINDOW_SIZE = TRAINING_SET_SIZE\n\n_, test_y = ppl.splitCol(dataset[TRAINING_SET_SIZE:], NUM_FEATURES)\n\n# Create ML model\nffnn = Model(NUM_FEATURES, NUM_LABELS, CONFIG)\n\nprint(\"Done\")", 
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "##\n## BOOTSTRAP/BOOSTING TRAINING WITH LOO\n##\n\n# Train thread id\ntrain_id = str(uuid.uuid1())[:8]\n#train_id = \"0b4045ec\"\n\nlog.info(\"\".join([\"(\", train_id, \")\", \" Training model: \", CONFIG['model_desc'], \"(\",MODEL_ID,\") , Pipeline: \", PIPELINE_ID]))\n                  \ntry:\n    \n    testSetIndex = isect[-(TEST_SET_SIZE/2):]\n    predictions = np.array([]).reshape(0,2)\n    \n    existing1 = mi.get_predictions(MARKET1, MODEL_ID)\n    existing2 = mi.get_predictions(MARKET2, MODEL_ID)\n    existing_predictions = existing1.index.intersection(existing2.index)\n    resultIndex = testSetIndex.tz_localize(None).difference(existing_predictions)\n    prediction_idx = np.array([testSetIndex.get_loc(idx) for idx in resultIndex])     \n    labels_idx = ppl.interleave(pandas.DataFrame(prediction_idx*2), pandas.DataFrame(prediction_idx*2+1)).values.flatten()\n    \n    for i in prediction_idx:\n        print \"Training\",\n        dataIdx = i * 2 + TRAINING_SET_SIZE\n        training_set = dataset[dataIdx-WINDOW_SIZE:dataIdx]\n        test_set = dataset[dataIdx:dataIdx+2]\n        success = False\n        prediction = [-1, -1]\n        retry = 0\n        while ((not success) & (retry<TRN_CNF['training_retries'])):\n            try:\n                ## CHOOSE BOOTSTRAP OR BOOST\n                #results = mlutils.boostingTrain(ffnn, training_set, test_set, TRN_CNF['lamda'], TRN_CNF['iterations'], CONFIG['debug'])\n                results = mlutils.bootstrapTrain(ffnn, training_set, test_set, TRN_CNF['lamda'], TRN_CNF['iterations'], TRN_CNF['threshold'], CONFIG['debug'])\n                prediction = np.nanmean(results[\"test_predictions\"], axis=0)\n                predictions =  np.concatenate([predictions, prediction])    \n                success = True\n            except ValueError: \n                print \"Value error\"\n                log.error(\"\".join([tag(train_id, i), \"ValueError - Retrying...\"]))\n                retry = retry + 1\n        \n        if (not success):\n            log.error(\"Failed to train after several retries\")\n            break\n            \n        print \".\"\n\n        # Extract predictions and store them\n        p1, p2 = [pandas.DataFrame([mkt], index=testSetIndex[i:i+1]) for mkt in prediction]\n        \n        mi.put_predictions(p1, MARKET1, MODEL_ID, update=True)\n        mi.put_predictions(p2, MARKET2, MODEL_ID, update=True)\n        \n        if (True):\n            log.debug(\"\".join([tag(train_id, i), testSetIndex[i].isoformat(), \" \", MARKET1, \": \", str(p1.values[0])]))\n            log.debug(\"\".join([tag(train_id, i), testSetIndex[i].isoformat(), \" \", MARKET2, \": \", str(p2.values[0])]))\n            \n        # Progress statistics\n        res = mlutils.evaluate(predictions, test_y[labels_idx][:len(predictions)], TRN_CNF['threshold'])\n        log.info(\"\".join([tag(train_id, i), str(\"Results after %d iterations, %.2f precision, %.2f recall at %.2f threshold\" % (i+1, res[0], res[1], TRN_CNF['threshold']))]))   \n        metrics.send([{'name':'MI.precision', 'value':res[0].tolist()},{'name':'MI.recall', 'value':res[1].tolist()}])\n\n        # Backup precictions to filestore (deprecated)\n        x = 1\n        for mkt in ppl.deinterleave(pandas.DataFrame(predictions)):\n            mkt.index = resultIndex[:len(mkt)]\n            mkt.to_csv(\"results.csv\", header=False)\n            objStore.put_file('Experiment2', \"results.csv\", \"\".join([MODEL_ID, \"_\", str(x), \".csv\"]) )\n            x = x + 1\n\n        # Try to free memory\n        gc.collect()\nexcept:\n    log.error(\"\".join([tag(train_id, i), str(\"Unexpected error: %s\" % sys.exc_info()[0])]))\n    raise\n    ", 
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {}
        }
    ], 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 2.0 (Deprecated)", 
            "name": "python2-spark20", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "2.7.11", 
            "name": "python", 
            "pygments_lexer": "ipython2", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }
        }
    }
}