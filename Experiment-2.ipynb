{
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 2.0", 
            "language": "python", 
            "name": "python2-spark20"
        }
    }, 
    "nbformat": 4, 
    "cells": [
        {
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Overwriting bluemix.py\n", 
                    "name": "stdout"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 60, 
            "source": "%%writefile bluemix.py\n\n###################\n## Bluemix Setup ##\n###################\n\nfrom io import StringIO\nimport requests\nimport json\nimport logmet\n\ndef get_metrics_client(metrics_cred):\n    #metrics = logmet.Logmet(\n    #    logmet_host='metrics.ng.bluemix.net',\n    #    logmet_port=9095,\n    #    space_id='xxx',\n    #    token='xxx'\n    #)\n    return\n\ndef get_logging_client(logging_cred):\n    return logmet.Logmet(\n        logmet_host=logging_cred['logmet_host'],\n        logmet_port=logging_cred['logmet_port'],\n        space_id=logging_cred['space_id'],\n        token=logging_cred['token']\n    )\n\nclass ObjectStore:\n    \n    def __init__(self, credentials_file):\n        credentials = json.load(open(credentials_file))\n        self.load_obj_storage_token(credentials)\n    \n    def load_obj_storage_token(self, obj_storage_cred):\n    \n        url = ''.join([obj_storage_cred['auth_url'], '/v3/auth/tokens'])\n        data = {'auth': {'identity': {'methods': ['password'],\n                'password': {'user': {'name': obj_storage_cred['username'],'domain': {'id': obj_storage_cred['domainId']},\n                'password': obj_storage_cred['password']}}}}}\n        headers = {'Content-Type': 'application/json'}\n        resp = requests.post(url=url, data=json.dumps(data), headers=headers)\n        resp_body = resp.json()\n        for e1 in resp_body['token']['catalog']:\n            if(e1['type']=='object-store'):\n                for e2 in e1['endpoints']:\n                            if(e2['interface']=='public'and e2['region']=='dallas'):\n                                endpoint_url = e2['url']\n        token = resp.headers['x-subject-token']\n\n        self.endpoint_url = endpoint_url\n        self.token = token\n        \n    def put_file(self, container, local_file_name):  \n        \"\"\"This functions returns a StringIO object containing\n        the file content from Bluemix Object Storage V3.\"\"\"\n\n        f = open(local_file_name,'r')    \n        headers = {'X-Auth-Token': self.token, 'accept': 'application/json'}\n        url = \"\".join([self.endpoint_url, \"/\", container, \"/\", filename])\n        resp = requests.put(url=url, headers=headers, data = f.read() )\n        print resp.text\n\n    def get_file(self, container, filename):\n        \"\"\"This functions returns a StringIO object containing\n        the file content from Bluemix Object Storage.\"\"\"\n\n        url = \"\".join([self.endpoint_url, \"/\", container, \"/\", filename])\n        headers = {'X-Auth-Token': self.token, 'accept': 'application/json'}\n        resp = requests.get(url=url, headers=headers)\n        return StringIO(resp.text)\n    \nclass MarketInsights:\n    \n    def __init__(self, credentials_file):\n        credentials = json.load(open(credentials_file))\n        self.credentials = credentials\n        \n    def put_dataset(self, dataset):\n        headers = { \\\n                   'X-IBM-Client-Id': self.credentials[\"clientId\"], \\\n                   'X-IBM-Client-Secret': self.credentials[\"clientSecret\"], \\\n                   'accept': 'application/json' \\\n                  }        \n        resp = requests.put(url=self.credentials.endpoint, headers=headers, data=dataset)        \n        return StringIO(resp.text)\n    \n    def get_dataset(self, market, pipelineId):        \n        headers = { \\\n                   'X-IBM-Client-Id': self.credentials[\"clientId\"], \\\n                   'X-IBM-Client-Secret': self.credentials[\"clientSecret\"], \\\n                   'accept': 'application/json' \\\n                  }        \n        query = { \\\n                 'where': { \\\n                          'market': market, \\\n                          'pipelineID': pipelineId \\\n                          } \\\n                }\n        url = \"\".join([self.credentials[\"endpoint\"],\"/miol-prod/api/v1/datasets?filter=\",json.dumps(query)])\n        resp = requests.get(url=url, headers=headers)   \n        print resp.text\n        return json.loads(resp.text)[0]\n    "
        }, 
        {
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "[{\"data\":[[1,2],[3,4]],\"id\":\"SPY_Pipeline1\",\"index\":[\"Thu Feb 15 2018 23:46:51 GMT+0000 (UTC)\"],\"market\":\"SPY\",\"pipelineID\":\"1\",\"_rev\":\"1-ce28706cd98f327e7659dae997fd9eed\"}]\n", 
                    "name": "stdout"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 61, 
            "source": "reload(bluemix)\nfrom bluemix import ObjectStore, MarketInsights\n\n#objStore = ObjectStore('object_storage_cred.json')\nmi = MarketInsights('MIOapi_cred.json')\n\ndataset = mi.get_dataset(\"SPY\", \"1\")\n"
        }, 
        {
            "metadata": {}, 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "execution_count": 67, 
                    "data": {
                        "text/plain": "   0  1\n0  1  2\n1  3  4", 
                        "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    }
                }
            ], 
            "cell_type": "code", 
            "execution_count": 67, 
            "source": "pandas.DataFrame(dataset[\"data\"])"
        }, 
        {
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 21, 
            "source": "#!pip install git+https://github.com/locke105/pylogmet.git\n\nimport pandas \nimport json\nimport gc\nimport sys\nfrom bluemix import *\n\n\nlogging_cred = json.load(open('logging_cred.json'))\n\ndataset = pandas.read_csv(objStore.get_file('Experiment2', 'Experiment2_zero.csv'), header=None)\n\ntestSetLength = 430\ntraining_set = dataset[:-(testSetLength)]\ntest_set = dataset[-(testSetLength):]"
        }, 
        {
            "metadata": {}, 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "execution_count": 22, 
                    "data": {
                        "text/plain": "            0         1         2         3         4         5         6   \\\n0     0.003571  0.575000  0.000000  0.367857  0.360714  1.000000  0.146429   \n1     0.390661  0.826167  0.199723  0.508553  0.503930  1.000000  0.000000   \n2     0.766138  1.000000  0.585742  0.943056  0.943056  0.943925  0.000000   \n3     0.769006  1.000000  0.681287  0.938596  0.938596  0.940058  0.000000   \n4     0.129534  0.953368  0.000000  0.528497  0.533679  1.000000  0.435233   \n5     0.096931  1.000000  0.000000  0.631664  0.632741  0.905223  0.340334   \n6     0.072423  0.504178  0.000000  0.270195  0.270195  1.000000  0.256267   \n7     0.078826  0.468008  0.000000  0.337731  0.337401  1.000000  0.253628   \n8     0.000000  0.786280  0.000000  0.736148  0.736148  1.000000  0.358839   \n9     0.000000  0.747823  0.000000  0.685475  0.684779  1.000000  0.344479   \n10    0.703476  0.961984  0.383418  0.772629  0.773353  1.000000  0.000000   \n11    0.517730  0.968085  0.280142  0.673759  0.677305  1.000000  0.000000   \n12    0.174452  0.433290  0.000000  0.425806  0.425032  1.000000  0.084129   \n13    0.114350  0.369955  0.000000  0.369955  0.356502  1.000000  0.000000   \n14    0.693603  0.799904  0.000000  0.019721  0.020683  1.000000  0.019240   \n15    0.390041  0.796680  0.000000  0.128631  0.136929  1.000000  0.091286   \n16    0.254461  0.703783  0.000000  0.312991  0.313704  1.000000  0.151677   \n17    0.239796  0.551020  0.000000  0.265306  0.275510  1.000000  0.201531   \n18    0.444072  0.641996  0.000000  0.046551  0.047890  1.000000  0.047890   \n19    0.649007  0.894040  0.000000  0.016556  0.023179  1.000000  0.023179   \n20    0.254765  0.997675  0.000000  0.745235  0.744305  1.000000  0.330544   \n21    0.076923  0.829431  0.000000  0.725753  0.722408  1.000000  0.394649   \n22    0.149401  0.740760  0.000000  0.740760  0.740500  1.000000  0.643155   \n23    0.120098  0.691176  0.000000  0.691176  0.696078  1.000000  0.610294   \n24    0.000000  0.795775  0.000000  0.510563  0.507042  1.000000  0.496479   \n25    0.005563  0.653222  0.000000  0.280019  0.279091  1.000000  0.271210   \n26    0.005376  0.551075  0.000000  0.333333  0.330645  1.000000  0.241935   \n27    0.046053  0.633772  0.000000  0.198684  0.197368  1.000000  0.092982   \n28    0.000000  0.450185  0.000000  0.380074  0.387454  1.000000  0.132841   \n29    0.276490  0.761589  0.210265  0.523731  0.525938  1.000000  0.000000   \n...        ...       ...       ...       ...       ...       ...       ...   \n2126  0.970426  1.000000  0.280757  0.602524  0.602918  0.811514  0.000000   \n2127  0.848297  1.000000  0.275542  0.455108  0.442724  0.696594  0.000000   \n2128  0.090504  0.722552  0.000000  0.722552  0.764095  1.000000  0.514837   \n2129  0.108396  0.683955  0.000000  0.683955  0.697761  1.000000  0.331157   \n2130  0.825096  1.000000  0.462671  0.655450  0.648791  0.998247  0.000000   \n2131  0.873267  1.000000  0.485149  0.546535  0.548515  0.758416  0.000000   \n2132  0.535483  0.658885  0.000000  0.557903  0.558644  1.000000  0.389661   \n2133  0.452055  0.579256  0.000000  0.403131  0.407045  1.000000  0.097847   \n2134  0.429362  0.842840  0.226715  0.587244  0.593020  1.000000  0.000000   \n2135  0.551060  0.793834  0.186898  0.535645  0.539499  1.000000  0.000000   \n2136  0.096846  0.554954  0.000000  0.537040  0.543758  1.000000  0.499160   \n2137  0.147177  0.516129  0.000000  0.479839  0.477823  1.000000  0.413306   \n2138  0.659720  1.000000  0.509163  0.778297  0.775782  0.776860  0.000000   \n2139  0.399038  1.000000  0.187500  0.966346  0.961538  0.975962  0.000000   \n2140  0.699019  1.000000  0.000000  0.467704  0.471086  0.803855  0.178221   \n2141  0.698225  1.000000  0.000000  0.784024  0.789941  0.884615  0.266272   \n2142  0.935316  1.000000  0.680253  0.736076  0.736076  0.744177  0.000000   \n2143  0.655405  1.000000  0.361486  0.597973  0.604730  0.810811  0.000000   \n2144  0.042414  0.598695  0.000000  0.595432  0.613377  1.000000  0.296900   \n2145  0.062108  0.575966  0.000000  0.520736  0.520736  1.000000  0.162047   \n2146  0.708431  0.924473  0.000000  0.766979  0.765808  1.000000  0.043911   \n2147  0.395257  0.723320  0.000000  0.683794  0.687747  1.000000  0.162055   \n2148  0.391756  1.000000  0.299822  0.761269  0.761269  0.761269  0.000000   \n2149  0.474359  1.000000  0.351282  0.879487  0.876923  0.876923  0.000000   \n2150  0.998788  1.000000  0.128485  0.252525  0.249697  0.701818  0.000000   \n2151  0.996885  1.000000  0.386293  0.467290  0.464174  0.626168  0.000000   \n2152  1.000000  1.000000  0.502722  0.573762  0.577080  0.736600  0.000000   \n2153  1.000000  1.000000  0.505102  0.561224  0.563138  0.708546  0.000000   \n2154  0.159303  0.476042  0.000000  0.380212  0.410703  1.000000  0.395769   \n2155  0.190161  0.380185  0.000000  0.286206  0.286964  1.000000  0.286964   \n\n            7         8   9   10  \n0     0.821429  0.043371   1   0  \n1     0.629219  0.040237   1   0  \n2     0.486633  0.085589   0   1  \n3     0.364035  0.105948   0   1  \n4     1.000000  0.029895   1   0  \n5     0.847604  0.034544   1   0  \n6     0.952646  0.055607   1   0  \n7     0.984499  0.056402   1   0  \n8     0.482850  0.058705   1   0  \n9     0.518635  0.053407   1   0  \n10    0.005069  0.051379   1   0  \n11    0.003546  0.043680   1   0  \n12    0.888516  0.072084   1   0  \n13    0.858744  0.069083   1   0  \n14    0.774892  0.038674   1   0  \n15    0.726141  0.037330   1   0  \n16    0.835475  0.052123   0   1  \n17    0.798469  0.060719   0   1  \n18    0.963831  0.055546   1   0  \n19    0.950331  0.046778   1   0  \n20    0.808926  0.040013   0   1  \n21    0.969900  0.046314   0   1  \n22    0.692087  0.071470   0   1  \n23    0.637255  0.063197   0   1  \n24    0.714789  0.043990   1   0  \n25    0.677330  0.040125   1   0  \n26    0.975806  0.057621   1   0  \n27    0.961842  0.042413   1   0  \n28    0.963100  0.041976   0   1  \n29    0.950331  0.033707   0   1  \n...        ...       ...  ..  ..  \n2126  0.308360  0.047177   1   0  \n2127  0.297214  0.050031   1   0  \n2128  0.643917  0.104399   0   1  \n2129  0.500000  0.099711   0   1  \n2130  0.216965  0.053074   1   0  \n2131  0.126733  0.078222   1   0  \n2132  0.889939  0.100400   1   0  \n2133  0.847358  0.079151   1   0  \n2134  0.578821  0.077295   1   0  \n2135  0.539499  0.080390   1   0  \n2136  0.737078  0.099693   1   0  \n2137  0.766129  0.076828   1   0  \n2138  0.421128  0.051772   1   0  \n2139  0.533654  0.032218   1   0  \n2140  0.508962  0.055009   0   1  \n2141  0.562130  0.052354   0   1  \n2142  0.437975  0.146963   1   0  \n2143  0.635135  0.045849   1   0  \n2144  0.539967  0.094950   1   0  \n2145  0.557556  0.091954   1   0  \n2146  0.584895  0.031774   1   0  \n2147  0.628458  0.039188   1   0  \n2148  0.098458  0.062729   1   0  \n2149  0.138462  0.060409   1   0  \n2150  0.124444  0.046042   1   0  \n2151  0.174455  0.049721   1   0  \n2152  0.313766  0.218658   0   1  \n2153  0.281888  0.242875   0   1  \n2154  0.667704  0.248916   0   1  \n2155  0.675692  0.270002   0   1  \n\n[2156 rows x 11 columns]", 
                        "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.003571</td>\n      <td>0.575000</td>\n      <td>0.000000</td>\n      <td>0.367857</td>\n      <td>0.360714</td>\n      <td>1.000000</td>\n      <td>0.146429</td>\n      <td>0.821429</td>\n      <td>0.043371</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.390661</td>\n      <td>0.826167</td>\n      <td>0.199723</td>\n      <td>0.508553</td>\n      <td>0.503930</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.629219</td>\n      <td>0.040237</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.766138</td>\n      <td>1.000000</td>\n      <td>0.585742</td>\n      <td>0.943056</td>\n      <td>0.943056</td>\n      <td>0.943925</td>\n      <td>0.000000</td>\n      <td>0.486633</td>\n      <td>0.085589</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.769006</td>\n      <td>1.000000</td>\n      <td>0.681287</td>\n      <td>0.938596</td>\n      <td>0.938596</td>\n      <td>0.940058</td>\n      <td>0.000000</td>\n      <td>0.364035</td>\n      <td>0.105948</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.129534</td>\n      <td>0.953368</td>\n      <td>0.000000</td>\n      <td>0.528497</td>\n      <td>0.533679</td>\n      <td>1.000000</td>\n      <td>0.435233</td>\n      <td>1.000000</td>\n      <td>0.029895</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.096931</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.631664</td>\n      <td>0.632741</td>\n      <td>0.905223</td>\n      <td>0.340334</td>\n      <td>0.847604</td>\n      <td>0.034544</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.072423</td>\n      <td>0.504178</td>\n      <td>0.000000</td>\n      <td>0.270195</td>\n      <td>0.270195</td>\n      <td>1.000000</td>\n      <td>0.256267</td>\n      <td>0.952646</td>\n      <td>0.055607</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.078826</td>\n      <td>0.468008</td>\n      <td>0.000000</td>\n      <td>0.337731</td>\n      <td>0.337401</td>\n      <td>1.000000</td>\n      <td>0.253628</td>\n      <td>0.984499</td>\n      <td>0.056402</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.000000</td>\n      <td>0.786280</td>\n      <td>0.000000</td>\n      <td>0.736148</td>\n      <td>0.736148</td>\n      <td>1.000000</td>\n      <td>0.358839</td>\n      <td>0.482850</td>\n      <td>0.058705</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.000000</td>\n      <td>0.747823</td>\n      <td>0.000000</td>\n      <td>0.685475</td>\n      <td>0.684779</td>\n      <td>1.000000</td>\n      <td>0.344479</td>\n      <td>0.518635</td>\n      <td>0.053407</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.703476</td>\n      <td>0.961984</td>\n      <td>0.383418</td>\n      <td>0.772629</td>\n      <td>0.773353</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.005069</td>\n      <td>0.051379</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.517730</td>\n      <td>0.968085</td>\n      <td>0.280142</td>\n      <td>0.673759</td>\n      <td>0.677305</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.003546</td>\n      <td>0.043680</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.174452</td>\n      <td>0.433290</td>\n      <td>0.000000</td>\n      <td>0.425806</td>\n      <td>0.425032</td>\n      <td>1.000000</td>\n      <td>0.084129</td>\n      <td>0.888516</td>\n      <td>0.072084</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.114350</td>\n      <td>0.369955</td>\n      <td>0.000000</td>\n      <td>0.369955</td>\n      <td>0.356502</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.858744</td>\n      <td>0.069083</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.693603</td>\n      <td>0.799904</td>\n      <td>0.000000</td>\n      <td>0.019721</td>\n      <td>0.020683</td>\n      <td>1.000000</td>\n      <td>0.019240</td>\n      <td>0.774892</td>\n      <td>0.038674</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.390041</td>\n      <td>0.796680</td>\n      <td>0.000000</td>\n      <td>0.128631</td>\n      <td>0.136929</td>\n      <td>1.000000</td>\n      <td>0.091286</td>\n      <td>0.726141</td>\n      <td>0.037330</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.254461</td>\n      <td>0.703783</td>\n      <td>0.000000</td>\n      <td>0.312991</td>\n      <td>0.313704</td>\n      <td>1.000000</td>\n      <td>0.151677</td>\n      <td>0.835475</td>\n      <td>0.052123</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.239796</td>\n      <td>0.551020</td>\n      <td>0.000000</td>\n      <td>0.265306</td>\n      <td>0.275510</td>\n      <td>1.000000</td>\n      <td>0.201531</td>\n      <td>0.798469</td>\n      <td>0.060719</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.444072</td>\n      <td>0.641996</td>\n      <td>0.000000</td>\n      <td>0.046551</td>\n      <td>0.047890</td>\n      <td>1.000000</td>\n      <td>0.047890</td>\n      <td>0.963831</td>\n      <td>0.055546</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.649007</td>\n      <td>0.894040</td>\n      <td>0.000000</td>\n      <td>0.016556</td>\n      <td>0.023179</td>\n      <td>1.000000</td>\n      <td>0.023179</td>\n      <td>0.950331</td>\n      <td>0.046778</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.254765</td>\n      <td>0.997675</td>\n      <td>0.000000</td>\n      <td>0.745235</td>\n      <td>0.744305</td>\n      <td>1.000000</td>\n      <td>0.330544</td>\n      <td>0.808926</td>\n      <td>0.040013</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.076923</td>\n      <td>0.829431</td>\n      <td>0.000000</td>\n      <td>0.725753</td>\n      <td>0.722408</td>\n      <td>1.000000</td>\n      <td>0.394649</td>\n      <td>0.969900</td>\n      <td>0.046314</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.149401</td>\n      <td>0.740760</td>\n      <td>0.000000</td>\n      <td>0.740760</td>\n      <td>0.740500</td>\n      <td>1.000000</td>\n      <td>0.643155</td>\n      <td>0.692087</td>\n      <td>0.071470</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.120098</td>\n      <td>0.691176</td>\n      <td>0.000000</td>\n      <td>0.691176</td>\n      <td>0.696078</td>\n      <td>1.000000</td>\n      <td>0.610294</td>\n      <td>0.637255</td>\n      <td>0.063197</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.000000</td>\n      <td>0.795775</td>\n      <td>0.000000</td>\n      <td>0.510563</td>\n      <td>0.507042</td>\n      <td>1.000000</td>\n      <td>0.496479</td>\n      <td>0.714789</td>\n      <td>0.043990</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.005563</td>\n      <td>0.653222</td>\n      <td>0.000000</td>\n      <td>0.280019</td>\n      <td>0.279091</td>\n      <td>1.000000</td>\n      <td>0.271210</td>\n      <td>0.677330</td>\n      <td>0.040125</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.005376</td>\n      <td>0.551075</td>\n      <td>0.000000</td>\n      <td>0.333333</td>\n      <td>0.330645</td>\n      <td>1.000000</td>\n      <td>0.241935</td>\n      <td>0.975806</td>\n      <td>0.057621</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.046053</td>\n      <td>0.633772</td>\n      <td>0.000000</td>\n      <td>0.198684</td>\n      <td>0.197368</td>\n      <td>1.000000</td>\n      <td>0.092982</td>\n      <td>0.961842</td>\n      <td>0.042413</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.000000</td>\n      <td>0.450185</td>\n      <td>0.000000</td>\n      <td>0.380074</td>\n      <td>0.387454</td>\n      <td>1.000000</td>\n      <td>0.132841</td>\n      <td>0.963100</td>\n      <td>0.041976</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.276490</td>\n      <td>0.761589</td>\n      <td>0.210265</td>\n      <td>0.523731</td>\n      <td>0.525938</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.950331</td>\n      <td>0.033707</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2126</th>\n      <td>0.970426</td>\n      <td>1.000000</td>\n      <td>0.280757</td>\n      <td>0.602524</td>\n      <td>0.602918</td>\n      <td>0.811514</td>\n      <td>0.000000</td>\n      <td>0.308360</td>\n      <td>0.047177</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2127</th>\n      <td>0.848297</td>\n      <td>1.000000</td>\n      <td>0.275542</td>\n      <td>0.455108</td>\n      <td>0.442724</td>\n      <td>0.696594</td>\n      <td>0.000000</td>\n      <td>0.297214</td>\n      <td>0.050031</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2128</th>\n      <td>0.090504</td>\n      <td>0.722552</td>\n      <td>0.000000</td>\n      <td>0.722552</td>\n      <td>0.764095</td>\n      <td>1.000000</td>\n      <td>0.514837</td>\n      <td>0.643917</td>\n      <td>0.104399</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2129</th>\n      <td>0.108396</td>\n      <td>0.683955</td>\n      <td>0.000000</td>\n      <td>0.683955</td>\n      <td>0.697761</td>\n      <td>1.000000</td>\n      <td>0.331157</td>\n      <td>0.500000</td>\n      <td>0.099711</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2130</th>\n      <td>0.825096</td>\n      <td>1.000000</td>\n      <td>0.462671</td>\n      <td>0.655450</td>\n      <td>0.648791</td>\n      <td>0.998247</td>\n      <td>0.000000</td>\n      <td>0.216965</td>\n      <td>0.053074</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2131</th>\n      <td>0.873267</td>\n      <td>1.000000</td>\n      <td>0.485149</td>\n      <td>0.546535</td>\n      <td>0.548515</td>\n      <td>0.758416</td>\n      <td>0.000000</td>\n      <td>0.126733</td>\n      <td>0.078222</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2132</th>\n      <td>0.535483</td>\n      <td>0.658885</td>\n      <td>0.000000</td>\n      <td>0.557903</td>\n      <td>0.558644</td>\n      <td>1.000000</td>\n      <td>0.389661</td>\n      <td>0.889939</td>\n      <td>0.100400</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2133</th>\n      <td>0.452055</td>\n      <td>0.579256</td>\n      <td>0.000000</td>\n      <td>0.403131</td>\n      <td>0.407045</td>\n      <td>1.000000</td>\n      <td>0.097847</td>\n      <td>0.847358</td>\n      <td>0.079151</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2134</th>\n      <td>0.429362</td>\n      <td>0.842840</td>\n      <td>0.226715</td>\n      <td>0.587244</td>\n      <td>0.593020</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.578821</td>\n      <td>0.077295</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2135</th>\n      <td>0.551060</td>\n      <td>0.793834</td>\n      <td>0.186898</td>\n      <td>0.535645</td>\n      <td>0.539499</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.539499</td>\n      <td>0.080390</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2136</th>\n      <td>0.096846</td>\n      <td>0.554954</td>\n      <td>0.000000</td>\n      <td>0.537040</td>\n      <td>0.543758</td>\n      <td>1.000000</td>\n      <td>0.499160</td>\n      <td>0.737078</td>\n      <td>0.099693</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2137</th>\n      <td>0.147177</td>\n      <td>0.516129</td>\n      <td>0.000000</td>\n      <td>0.479839</td>\n      <td>0.477823</td>\n      <td>1.000000</td>\n      <td>0.413306</td>\n      <td>0.766129</td>\n      <td>0.076828</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2138</th>\n      <td>0.659720</td>\n      <td>1.000000</td>\n      <td>0.509163</td>\n      <td>0.778297</td>\n      <td>0.775782</td>\n      <td>0.776860</td>\n      <td>0.000000</td>\n      <td>0.421128</td>\n      <td>0.051772</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2139</th>\n      <td>0.399038</td>\n      <td>1.000000</td>\n      <td>0.187500</td>\n      <td>0.966346</td>\n      <td>0.961538</td>\n      <td>0.975962</td>\n      <td>0.000000</td>\n      <td>0.533654</td>\n      <td>0.032218</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2140</th>\n      <td>0.699019</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.467704</td>\n      <td>0.471086</td>\n      <td>0.803855</td>\n      <td>0.178221</td>\n      <td>0.508962</td>\n      <td>0.055009</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2141</th>\n      <td>0.698225</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.784024</td>\n      <td>0.789941</td>\n      <td>0.884615</td>\n      <td>0.266272</td>\n      <td>0.562130</td>\n      <td>0.052354</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2142</th>\n      <td>0.935316</td>\n      <td>1.000000</td>\n      <td>0.680253</td>\n      <td>0.736076</td>\n      <td>0.736076</td>\n      <td>0.744177</td>\n      <td>0.000000</td>\n      <td>0.437975</td>\n      <td>0.146963</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2143</th>\n      <td>0.655405</td>\n      <td>1.000000</td>\n      <td>0.361486</td>\n      <td>0.597973</td>\n      <td>0.604730</td>\n      <td>0.810811</td>\n      <td>0.000000</td>\n      <td>0.635135</td>\n      <td>0.045849</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2144</th>\n      <td>0.042414</td>\n      <td>0.598695</td>\n      <td>0.000000</td>\n      <td>0.595432</td>\n      <td>0.613377</td>\n      <td>1.000000</td>\n      <td>0.296900</td>\n      <td>0.539967</td>\n      <td>0.094950</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2145</th>\n      <td>0.062108</td>\n      <td>0.575966</td>\n      <td>0.000000</td>\n      <td>0.520736</td>\n      <td>0.520736</td>\n      <td>1.000000</td>\n      <td>0.162047</td>\n      <td>0.557556</td>\n      <td>0.091954</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2146</th>\n      <td>0.708431</td>\n      <td>0.924473</td>\n      <td>0.000000</td>\n      <td>0.766979</td>\n      <td>0.765808</td>\n      <td>1.000000</td>\n      <td>0.043911</td>\n      <td>0.584895</td>\n      <td>0.031774</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2147</th>\n      <td>0.395257</td>\n      <td>0.723320</td>\n      <td>0.000000</td>\n      <td>0.683794</td>\n      <td>0.687747</td>\n      <td>1.000000</td>\n      <td>0.162055</td>\n      <td>0.628458</td>\n      <td>0.039188</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2148</th>\n      <td>0.391756</td>\n      <td>1.000000</td>\n      <td>0.299822</td>\n      <td>0.761269</td>\n      <td>0.761269</td>\n      <td>0.761269</td>\n      <td>0.000000</td>\n      <td>0.098458</td>\n      <td>0.062729</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2149</th>\n      <td>0.474359</td>\n      <td>1.000000</td>\n      <td>0.351282</td>\n      <td>0.879487</td>\n      <td>0.876923</td>\n      <td>0.876923</td>\n      <td>0.000000</td>\n      <td>0.138462</td>\n      <td>0.060409</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2150</th>\n      <td>0.998788</td>\n      <td>1.000000</td>\n      <td>0.128485</td>\n      <td>0.252525</td>\n      <td>0.249697</td>\n      <td>0.701818</td>\n      <td>0.000000</td>\n      <td>0.124444</td>\n      <td>0.046042</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2151</th>\n      <td>0.996885</td>\n      <td>1.000000</td>\n      <td>0.386293</td>\n      <td>0.467290</td>\n      <td>0.464174</td>\n      <td>0.626168</td>\n      <td>0.000000</td>\n      <td>0.174455</td>\n      <td>0.049721</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2152</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.502722</td>\n      <td>0.573762</td>\n      <td>0.577080</td>\n      <td>0.736600</td>\n      <td>0.000000</td>\n      <td>0.313766</td>\n      <td>0.218658</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2153</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.505102</td>\n      <td>0.561224</td>\n      <td>0.563138</td>\n      <td>0.708546</td>\n      <td>0.000000</td>\n      <td>0.281888</td>\n      <td>0.242875</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2154</th>\n      <td>0.159303</td>\n      <td>0.476042</td>\n      <td>0.000000</td>\n      <td>0.380212</td>\n      <td>0.410703</td>\n      <td>1.000000</td>\n      <td>0.395769</td>\n      <td>0.667704</td>\n      <td>0.248916</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2155</th>\n      <td>0.190161</td>\n      <td>0.380185</td>\n      <td>0.000000</td>\n      <td>0.286206</td>\n      <td>0.286964</td>\n      <td>1.000000</td>\n      <td>0.286964</td>\n      <td>0.675692</td>\n      <td>0.270002</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>2156 rows \u00d7 11 columns</p>\n</div>"
                    }
                }
            ], 
            "cell_type": "code", 
            "execution_count": 22, 
            "source": "dataset"
        }, 
        {
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Done\n", 
                    "name": "stdout"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 4, 
            "source": "import tensorflow as tf\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\n##### Specific to the data ##\nNUM_FEATURES = (2 * 4) + 1\nNUM_LABELS = 2\n#############################\n\n\n##### #Define the architecture\nHIDDEN_UNITS = 32\n\n# The random seed that defines initialization.\nSEED = 42\n\n# This is where training samples and labels are fed to the graph.\n# These placeholder nodes will be fed a batch of training data at each\n# training step, which we'll write once we define the graph structure.\ntrain_data_node = tf.placeholder(tf.float32, shape=(None, NUM_FEATURES))\ntrain_labels_node = tf.placeholder(tf.float32, shape=(None, NUM_LABELS))\nlam = tf.placeholder(tf.float32)\n\n# The variables below hold all the trainable weights. For each, the\n# parameter defines how the variables will be initialized. \n# TODO : These should be pulled from a config file\n\nTheta1 = tf.Variable( tf.truncated_normal([HIDDEN_UNITS, (NUM_FEATURES)], stddev=0.1))\n\nTheta2 = tf.Variable( tf.truncated_normal([NUM_LABELS, HIDDEN_UNITS],stddev=0.1))\nbias2 = tf.Variable(tf.constant(0.1, shape=[NUM_LABELS]))\n    \nprint('Done')"
        }, 
        {
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Done\n", 
                    "name": "stdout"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 5, 
            "source": "def model(X, Theta1, Theta2, bias):\n    \"\"\"The Model definition.\"\"\"\n    # Perceptron\n    \n    layer1 = tf.nn.sigmoid(tf.matmul(X, tf.transpose(Theta1)))\n                        \n    output = tf.nn.bias_add(tf.matmul(layer1, tf.transpose(Theta2)),bias)\n\n    return output\n    \nprint('Done')"
        }, 
        {
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Done\n", 
                    "name": "stdout"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 6, 
            "source": "yhat = model(train_data_node, Theta1, Theta2, bias2)\n\n# Change the weights by subtracting derivative with respect to that weight\nloss =  tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=train_labels_node, logits=yhat))\n# Regularization using L2 Loss function \nregularizer = tf.nn.l2_loss(Theta1) + tf.nn.l2_loss(Theta2)\nreg = (lam / tf.to_float(tf.shape(train_labels_node)[0])) * regularizer\nloss_reg = loss + reg\n\n# Optimizer: \n\n# Gradient Descent\noptimizer = tf.contrib.opt.ScipyOptimizerInterface(loss_reg, options={'maxiter':4000})\n#update_weights = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n\n# Predictions\ntrain_prediction = tf.sigmoid(yhat)\n\nprint('Done')"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 7, 
            "source": "# Keep track of the loss at each iteration so we can chart it later\nJ = []\n\ndef loss_callback():\n    print \"Recalc...\"\n    \ndef step_callback(params):\n    J.append(params)\n    \ndef split(data, num_features):    \n    return data.values[:,:num_features], data.values[:,num_features:]\n    \ndef sample(training_set, method=\"RANDOM\", prop=.9, loo=0, boost = []): \n    if (method == \"RANDOM\"):\n        training_set = training_set.sample(frac=1).reset_index(drop=True)\n        idx = np.arange(0,len(training_set)) / float(len(training_set))\n        return [training_set[idx<prop], training_set[idx>=prop]]\n    elif (method == \"LOO\"):\n        idx = np.array(range(0,len(training_set)))\n        return [training_set[idx!=loo], training_set[idx==loo]]\n    elif (method == \"BOOTSTRAP\"):\n        idx = np.array(range(0,len(training_set)))\n        sample = np.random.choice(idx, len(training_set), replace=True)\n        return pandas.DataFrame(training_set.values[sample,:]), training_set[~np.in1d(idx, sample)]\n    elif (method == \"BOOSTING\"):\n        idx = np.array(range(0,len(training_set)))\n        sample = np.random.choice(idx, len(training_set), replace=True, p=boost)\n        return pandas.DataFrame(training_set.values[sample,:]), training_set[~np.in1d(idx, sample)]\n    \ndef minimize(feed_dict, train=True):\n    \n    #optimizer.minimize(feed_dict=feed_dict, fetches=[loss_reg], loss_callback=loss_callback)\n    if (train):\n        optimizer.minimize(feed_dict=feed_dict)\n\n    return loss.eval(feed_dict), train_prediction.eval(feed_dict)\n\ndef evaluate(predictions, data_y, threshold):\n    a = np.argmax(predictions,axis=1) \n    b = np.argmax(data_y,axis=1) \n    a = a[(predictions > threshold).any(axis=1)]\n    b = b[(predictions > threshold).any(axis=1)]\n    precision = np.float32(np.sum(a == b) / np.float32(b.shape[0]))\n    recall = np.float32(np.sum(a == b) / np.float32(data_y.shape[0])) # Correct Recall\n    recall = np.float32(b.shape[0]) / data_y.shape[0] # Number of Days traded\n    F_score = (2.0 * precision * recall) / (precision + recall)\n    return precision, recall, F_score, predictions\n\ndef predict(data_X, data_y, lam1, threshold):    \n    loss, predictions = minimize({train_data_node: data_X, train_labels_node: data_y, lam: lam1}, train=False)\n    precision, recall, F_score, predictions = evaluate(predictions, data_y, threshold)\n    return loss, precision, recall, F_score, predictions\n    \n\ndef train(train_dict, val_dict, test_dict, threshold, iterations=50, debug=True):\n    \n    tf.logging.set_verbosity(tf.logging.ERROR)\n    \n    metrics = {\n        \"train_loss\":[],\n        \"train_precision\":[],\n        \"train_recall\":[],\n        \"train_f\":[],\n        \"val_loss\":[],\n        \"val_precision\":[],\n        \"val_recall\":[],\n        \"val_f\":[],\n        \"test_loss\":[],\n        \"test_precision\":[],\n        \"test_recall\":[],\n        \"test_f\":[],\n        \"test_predictions\":[]\n    }\n    \n    for i in range(0,iterations):\n        \n        for j in range(0, 50):\n            \n            # Create a new interactive session that we'll use in\n            # subsequent code cells.\n            s = tf.InteractiveSession()\n            s.as_default()\n\n            # Initialize all the variables we defined above.\n            tf.initialize_all_variables().run()\n\n            minimize(train_dict)\n            train_loss, train_precision, train_recall, train_f, _ = predict(train_dict[train_data_node], train_dict[train_labels_node], train_dict[lam], threshold)\n\n            if (train_loss < .65):\n                print \".\",\n                metrics[\"train_loss\"].append(train_loss)\n                metrics[\"train_precision\"].append(train_precision)\n                metrics[\"train_recall\"].append(train_recall)\n                metrics[\"train_f\"].append(train_f)\n\n                val_loss, val_precision, val_recall, val_f, _= predict(val_dict[train_data_node], val_dict[train_labels_node], val_dict[lam], threshold)\n\n                metrics[\"val_loss\"].append(val_loss)\n                metrics[\"val_precision\"].append(val_precision)\n                metrics[\"val_recall\"].append(val_recall)\n                metrics[\"val_f\"].append(val_f)\n                \n                test_loss, test_precision, test_recall, test_f, test_predictions = predict(test_dict[train_data_node], test_dict[train_labels_node], test_dict[lam], threshold)\n\n                metrics[\"test_loss\"].append(test_loss)\n                metrics[\"test_precision\"].append(test_precision)\n                metrics[\"test_recall\"].append(test_recall)\n                metrics[\"test_f\"].append(test_f)\n                metrics[\"test_predictions\"] = test_predictions # Return the last set of predictions (could return the one with the best val score)\n                del s\n                break;\n            else:\n                del s\n        \n        if (j >= 50):\n            print(\"ERROR : Failed to minimise function\")\n            \n    results = {\n        \"train_loss\": {\"mean\":np.nanmean(metrics[\"train_loss\"]), \"std\":np.nanstd(metrics[\"train_loss\"]), \"values\":metrics[\"train_loss\"]},\n        \"train_precision\": {\"mean\":np.nanmean(metrics[\"train_precision\"]), \"std\":np.nanstd(metrics[\"train_precision\"]), \"values\":metrics[\"train_precision\"]},\n        \"train_recall\": {\"mean\":np.nanmean(metrics[\"train_recall\"]), \"std\":np.nanstd(metrics[\"train_recall\"]), \"values\":metrics[\"train_recall\"]},\n        \"train_f\": {\"mean\":np.nanmean(metrics[\"train_f\"]), \"std\":np.nanstd(metrics[\"train_f\"]), \"values\":metrics[\"train_f\"]},\n        \"val_loss\": {\"mean\":np.nanmean(metrics[\"val_loss\"]), \"std\":np.nanstd(metrics[\"val_loss\"]), \"values\":metrics[\"val_loss\"]},\n        \"val_precision\":{\"mean\":np.nanmean(metrics[\"val_precision\"]), \"std\":np.nanstd(metrics[\"val_precision\"]), \"values\":metrics[\"val_precision\"]},\n        \"val_recall\": {\"mean\":np.nanmean(metrics[\"val_recall\"]), \"std\":np.nanstd(metrics[\"val_recall\"]), \"values\":metrics[\"val_recall\"]},\n        \"val_f\": {\"mean\":np.nanmean(metrics[\"val_f\"]), \"std\":np.nanstd(metrics[\"val_f\"]), \"values\":metrics[\"val_f\"]},\n        \"test_loss\": {\"mean\":np.nanmean(metrics[\"test_loss\"]), \"std\":np.nanstd(metrics[\"test_loss\"]), \"values\":metrics[\"test_loss\"]},\n        \"test_precision\":{\"mean\":np.nanmean(metrics[\"test_precision\"]), \"std\":np.nanstd(metrics[\"test_precision\"]), \"values\":metrics[\"test_precision\"]},\n        \"test_recall\": {\"mean\":np.nanmean(metrics[\"test_recall\"]), \"std\":np.nanstd(metrics[\"test_recall\"]), \"values\":metrics[\"test_recall\"]},\n        \"test_f\": {\"mean\":np.nanmean(metrics[\"test_f\"]), \"std\":np.nanstd(metrics[\"test_f\"]), \"values\":metrics[\"test_f\"]},\n        \"test_predictions\": metrics[\"test_predictions\"],\n    }\n    \n    print \".\",\n    if debug:\n        print(\"Iterations : %d Lambda : %.2f, Threshold : %.2f\" % (iterations, val_dict[lam], threshold))\n        print(\"Training loss : %.2f+/-%.2f, precision : %.2f+/-%.2f, recall : %.2f+/-%.2f, F : %.2f+/-%.2f\" % \n              (results[\"train_loss\"][\"mean\"], results[\"train_loss\"][\"std\"],\n               results[\"train_precision\"][\"mean\"], results[\"train_precision\"][\"std\"],\n               results[\"train_recall\"][\"mean\"], results[\"train_recall\"][\"std\"],\n               results[\"train_f\"][\"mean\"], results[\"train_f\"][\"std\"]))\n        print(\"Validation loss : %.2f+/-%.2f, precision : %.2f+/-%.2f, recall : %.2f+/-%.2f, F : %.2f+/-%.2f\" % \n              (results[\"val_loss\"][\"mean\"], results[\"val_loss\"][\"std\"],\n               results[\"val_precision\"][\"mean\"], results[\"val_precision\"][\"std\"],\n               results[\"val_recall\"][\"mean\"], results[\"val_recall\"][\"std\"],\n               results[\"val_f\"][\"mean\"], results[\"val_f\"][\"std\"]))\n        print(\"Test loss : %.2f+/-%.2f, precision : %.2f+/-%.2f, recall : %.2f+/-%.2f, F : %.2f+/-%.2f\" % \n              (results[\"test_loss\"][\"mean\"], results[\"test_loss\"][\"std\"],\n               results[\"test_precision\"][\"mean\"], results[\"test_precision\"][\"std\"],\n               results[\"test_recall\"][\"mean\"], results[\"test_recall\"][\"std\"],\n               results[\"test_f\"][\"mean\"], results[\"test_f\"][\"std\"]))\n\n    return results"
        }, 
        {
            "metadata": {
                "scrolled": false, 
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 8, 
            "source": "### \n### CROSS-VAL RANDOM SAMPLING\n###\n\ndef bootstrapTrain(training_set, test_set, lamda, iterations, threshold, debug=False):\n\n    metrics = {\n        \"train_loss\":[],\n        \"train_precision\":[],\n        \"train_recall\":[],\n        \"train_f\":[],\n        \"val_loss\":[],\n        \"val_precision\":[],\n        \"val_recall\":[],\n        \"val_f\":[],\n        \"test_loss\":[],\n        \"test_precision\":[],\n        \"test_recall\":[],\n        \"test_f\":[],\n        \"test_predictions\":[]\n    }\n    \n    test_X, test_y = split(test_set, NUM_FEATURES)\n\n    for i in range(0, iterations):\n        \n        print \".\",\n\n        train_sample, val_sample = sample(training_set, method=\"BOOTSTRAP\", loo=i)\n\n        train_sample_X, train_sample_y = split(train_sample, NUM_FEATURES)\n        val_sample_X, val_sample_y = split(val_sample, NUM_FEATURES)        \n\n        results = train({train_data_node: train_sample_X, train_labels_node: train_sample_y, lam: lamda}, {train_data_node: val_sample_X, train_labels_node: val_sample_y, lam: lamda}, {train_data_node: test_X, train_labels_node: test_y, lam: lamda}, threshold, 1, False)\n\n        metrics[\"train_loss\"].append(results[\"train_loss\"][\"mean\"])\n        metrics[\"train_precision\"].append(results[\"train_precision\"][\"mean\"])\n        metrics[\"train_recall\"].append(results[\"train_recall\"][\"mean\"])\n        metrics[\"train_f\"].append(results[\"train_f\"][\"mean\"])\n        metrics[\"val_loss\"].append(results[\"val_loss\"][\"mean\"])\n        metrics[\"val_precision\"].append(results[\"val_precision\"][\"mean\"])\n        metrics[\"val_recall\"].append(results[\"val_recall\"][\"mean\"])\n        metrics[\"val_f\"].append(results[\"val_f\"][\"mean\"])\n        metrics[\"test_loss\"].append(results[\"test_loss\"][\"mean\"])\n        metrics[\"test_precision\"].append(results[\"test_precision\"][\"mean\"])\n        metrics[\"test_recall\"].append(results[\"test_recall\"][\"mean\"])\n        metrics[\"test_f\"].append(results[\"test_f\"][\"mean\"])\n        metrics[\"test_predictions\"].append(results[\"test_predictions\"])  \n\n\n    results = {\n        \"train_loss\": {\"mean\":np.nanmean(metrics[\"train_loss\"]), \"std\":np.nanstd(metrics[\"train_loss\"]), \"values\":metrics[\"train_loss\"]},\n        \"train_precision\": {\"mean\":np.nanmean(metrics[\"train_precision\"]), \"std\":np.nanstd(metrics[\"train_precision\"]), \"values\":metrics[\"train_precision\"]},\n        \"train_recall\": {\"mean\":np.nanmean(metrics[\"train_recall\"]), \"std\":np.nanstd(metrics[\"train_recall\"]), \"values\":metrics[\"train_recall\"]},\n        \"train_f\": {\"mean\":np.nanmean(metrics[\"train_f\"]), \"std\":np.nanstd(metrics[\"train_f\"]), \"values\":metrics[\"train_f\"]},\n        \"val_loss\": {\"mean\":np.nanmean(metrics[\"val_loss\"]), \"std\":np.nanstd(metrics[\"val_loss\"]), \"values\":metrics[\"val_loss\"]},\n        \"val_precision\":{\"mean\":np.nanmean(metrics[\"val_precision\"]), \"std\":np.nanstd(metrics[\"val_precision\"]), \"values\":metrics[\"val_precision\"]},\n        \"val_recall\": {\"mean\":np.nanmean(metrics[\"val_recall\"]), \"std\":np.nanstd(metrics[\"val_recall\"]), \"values\":metrics[\"val_recall\"]},\n        \"val_f\": {\"mean\":np.nanmean(metrics[\"val_f\"]), \"std\":np.nanstd(metrics[\"val_f\"]), \"values\":metrics[\"val_f\"]},\n        \"test_loss\": {\"mean\":np.nanmean(metrics[\"test_loss\"]), \"std\":np.nanstd(metrics[\"test_loss\"]), \"values\":metrics[\"test_loss\"]},\n        \"test_precision\":{\"mean\":np.nanmean(metrics[\"test_precision\"]), \"std\":np.nanstd(metrics[\"test_precision\"]), \"values\":metrics[\"test_precision\"]},\n        \"test_recall\": {\"mean\":np.nanmean(metrics[\"test_recall\"]), \"std\":np.nanstd(metrics[\"test_recall\"]), \"values\":metrics[\"test_recall\"]},\n        \"test_f\": {\"mean\":np.nanmean(metrics[\"test_f\"]), \"std\":np.nanstd(metrics[\"test_f\"]), \"values\":metrics[\"test_f\"]},\n        \"test_predictions\": metrics[\"test_predictions\"],\n    }\n\n    if debug:\n        print(\"Iteration : %d Lambda : %.2f, Threshold : %.2f\" % (i, lamda, threshold))\n        print(\"Training loss : %.2f+/-%.2f, precision : %.2f+/-%.2f, recall : %.2f+/-%.2f, F : %.2f+/-%.2f\" % \n              (results[\"train_loss\"][\"mean\"], results[\"train_loss\"][\"std\"],\n               results[\"train_precision\"][\"mean\"], results[\"train_precision\"][\"std\"],\n               results[\"train_recall\"][\"mean\"], results[\"train_recall\"][\"std\"],\n               results[\"train_f\"][\"mean\"], results[\"train_f\"][\"std\"]))\n        print(\"Validation loss : %.2f+/-%.2f, precision : %.2f+/-%.2f, recall : %.2f+/-%.2f, F : %.2f+/-%.2f\" % \n              (results[\"val_loss\"][\"mean\"], results[\"val_loss\"][\"std\"],\n               results[\"val_precision\"][\"mean\"], results[\"val_precision\"][\"std\"],\n               results[\"val_recall\"][\"mean\"], results[\"val_recall\"][\"std\"],\n               results[\"val_f\"][\"mean\"], results[\"val_f\"][\"std\"]))\n        print(\"Test loss : %.2f+/-%.2f, precision : %.2f+/-%.2f, recall : %.2f+/-%.2f, F : %.2f+/-%.2f\" % \n              (results[\"test_loss\"][\"mean\"], results[\"test_loss\"][\"std\"],\n               results[\"test_precision\"][\"mean\"], results[\"test_precision\"][\"std\"],\n               results[\"test_recall\"][\"mean\"], results[\"test_recall\"][\"std\"],\n               results[\"test_f\"][\"mean\"], results[\"test_f\"][\"std\"]))\n\n    return results\n"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 9, 
            "source": "### \n### BOOSTING\n###\n\ndef boostingTrain(training_set, test_set, lamda, iterations, debug=False):\n\n    metrics = {\n        \"train_loss\":[],\n        \"train_precision\":[],\n        \"train_recall\":[],\n        \"train_f\":[],\n        \"val_loss\":[],\n        \"val_precision\":[],\n        \"val_recall\":[],\n        \"val_f\":[],\n        \"test_loss\":[],\n        \"test_precision\":[],\n        \"test_recall\":[],\n        \"test_f\":[],\n        \"test_predictions\":[]\n    }\n    \n    test_X, test_y = split(test_set, NUM_FEATURES)\n    train_X, train_y = split(training_set, NUM_FEATURES)\n    threshold = 0 # For boosting to work this must be 0\n    boost = np.array([1.0/len(training_set)] * len(training_set))\n\n    for i in range(0, iterations):\n        \n        print \".\",\n\n        train_sample, val_sample = sample(training_set, method=\"BOOSTING\", boost=boost)\n\n        train_sample_X, train_sample_y = split(train_sample, NUM_FEATURES)\n        val_sample_X, val_sample_y = split(val_sample, NUM_FEATURES)        \n\n        results = train({train_data_node: train_sample_X, train_labels_node: train_sample_y, lam: lamda}, {train_data_node: val_sample_X, train_labels_node: val_sample_y, lam: lamda}, {train_data_node: test_X, train_labels_node: test_y, lam: lamda}, threshold, 1, False)\n\n        #Evaluate the results and calculate the odds of misclassification\n        _, _, _, _, train_predictions = predict(train_X, train_y, lamda, threshold)\n        precision = np.argmax(train_predictions,axis=1) == np.argmax(train_y,axis=1)\n        epsilon = sum(boost[~precision]) \n        delta = epsilon / (1.0 - epsilon)\n        boost[precision] = boost[precision] * delta\n        boost = boost / sum(boost)\n        \n        \n        metrics[\"train_loss\"].append(results[\"train_loss\"][\"mean\"])\n        metrics[\"train_precision\"].append(results[\"train_precision\"][\"mean\"])\n        metrics[\"train_recall\"].append(results[\"train_recall\"][\"mean\"])\n        metrics[\"train_f\"].append(results[\"train_f\"][\"mean\"])\n        metrics[\"val_loss\"].append(results[\"val_loss\"][\"mean\"])\n        metrics[\"val_precision\"].append(results[\"val_precision\"][\"mean\"])\n        metrics[\"val_recall\"].append(results[\"val_recall\"][\"mean\"])\n        metrics[\"val_f\"].append(results[\"val_f\"][\"mean\"])\n        metrics[\"test_loss\"].append(results[\"test_loss\"][\"mean\"])\n        metrics[\"test_precision\"].append(results[\"test_precision\"][\"mean\"])\n        metrics[\"test_recall\"].append(results[\"test_recall\"][\"mean\"])\n        metrics[\"test_f\"].append(results[\"test_f\"][\"mean\"])\n        metrics[\"test_predictions\"].append(results[\"test_predictions\"])\n        \n\n\n    results = {\n        \"train_loss\": {\"mean\":np.nanmean(metrics[\"train_loss\"]), \"std\":np.nanstd(metrics[\"train_loss\"]), \"values\":metrics[\"train_loss\"]},\n        \"train_precision\": {\"mean\":np.nanmean(metrics[\"train_precision\"]), \"std\":np.nanstd(metrics[\"train_precision\"]), \"values\":metrics[\"train_precision\"]},\n        \"train_recall\": {\"mean\":np.nanmean(metrics[\"train_recall\"]), \"std\":np.nanstd(metrics[\"train_recall\"]), \"values\":metrics[\"train_recall\"]},\n        \"train_f\": {\"mean\":np.nanmean(metrics[\"train_f\"]), \"std\":np.nanstd(metrics[\"train_f\"]), \"values\":metrics[\"train_f\"]},\n        \"val_loss\": {\"mean\":np.nanmean(metrics[\"val_loss\"]), \"std\":np.nanstd(metrics[\"val_loss\"]), \"values\":metrics[\"val_loss\"]},\n        \"val_precision\":{\"mean\":np.nanmean(metrics[\"val_precision\"]), \"std\":np.nanstd(metrics[\"val_precision\"]), \"values\":metrics[\"val_precision\"]},\n        \"val_recall\": {\"mean\":np.nanmean(metrics[\"val_recall\"]), \"std\":np.nanstd(metrics[\"val_recall\"]), \"values\":metrics[\"val_recall\"]},\n        \"val_f\": {\"mean\":np.nanmean(metrics[\"val_f\"]), \"std\":np.nanstd(metrics[\"val_f\"]), \"values\":metrics[\"val_f\"]},\n        \"test_loss\": {\"mean\":np.nanmean(metrics[\"test_loss\"]), \"std\":np.nanstd(metrics[\"test_loss\"]), \"values\":metrics[\"test_loss\"]},\n        \"test_precision\":{\"mean\":np.nanmean(metrics[\"test_precision\"]), \"std\":np.nanstd(metrics[\"test_precision\"]), \"values\":metrics[\"test_precision\"]},\n        \"test_recall\": {\"mean\":np.nanmean(metrics[\"test_recall\"]), \"std\":np.nanstd(metrics[\"test_recall\"]), \"values\":metrics[\"test_recall\"]},\n        \"test_f\": {\"mean\":np.nanmean(metrics[\"test_f\"]), \"std\":np.nanstd(metrics[\"test_f\"]), \"values\":metrics[\"test_f\"]},\n        \"test_predictions\": metrics[\"test_predictions\"],\n        \"weights\":boost\n    }\n\n    if debug:\n        print(\"Iteration : %d Lambda : %.2f, Threshold : %.2f\" % (i, lamda, threshold))\n        print(\"Training loss : %.2f+/-%.2f, precision : %.2f+/-%.2f, recall : %.2f+/-%.2f, F : %.2f+/-%.2f\" % \n              (results[\"train_loss\"][\"mean\"], results[\"train_loss\"][\"std\"],\n               results[\"train_precision\"][\"mean\"], results[\"train_precision\"][\"std\"],\n               results[\"train_recall\"][\"mean\"], results[\"train_recall\"][\"std\"],\n               results[\"train_f\"][\"mean\"], results[\"train_f\"][\"std\"]))\n        print(\"Validation loss : %.2f+/-%.2f, precision : %.2f+/-%.2f, recall : %.2f+/-%.2f, F : %.2f+/-%.2f\" % \n              (results[\"val_loss\"][\"mean\"], results[\"val_loss\"][\"std\"],\n               results[\"val_precision\"][\"mean\"], results[\"val_precision\"][\"std\"],\n               results[\"val_recall\"][\"mean\"], results[\"val_recall\"][\"std\"],\n               results[\"val_f\"][\"mean\"], results[\"val_f\"][\"std\"]))\n        print(\"Test loss : %.2f+/-%.2f, precision : %.2f+/-%.2f, recall : %.2f+/-%.2f, F : %.2f+/-%.2f\" % \n              (results[\"test_loss\"][\"mean\"], results[\"test_loss\"][\"std\"],\n               results[\"test_precision\"][\"mean\"], results[\"test_precision\"][\"std\"],\n               results[\"test_recall\"][\"mean\"], results[\"test_recall\"][\"std\"],\n               results[\"test_f\"][\"mean\"], results[\"test_f\"][\"std\"]))\n\n    return results\n"
        }, 
        {
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nResults after 396 iterations, 0.00 precision, 1.00 recall at 0.00 threshold\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nResults after 398 iterations, 0.50 precision, 1.00 recall at 0.00 threshold\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nResults after 400 iterations, 0.67 precision, 1.00 recall at 0.00 threshold\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nResults after 402 iterations, 0.75 precision, 1.00 recall at 0.00 threshold\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nResults after 404 iterations, 0.60 precision, 1.00 recall at 0.00 threshold\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nResults after 406 iterations, 0.67 precision, 1.00 recall at 0.00 threshold\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nResults after 408 iterations, 0.64 precision, 1.00 recall at 0.00 threshold\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nResults after 410 iterations, 0.56 precision, 1.00 recall at 0.00 threshold\n\n. . . . . . . . . . . . . . . . . . .", 
                    "name": "stdout"
                }
            ], 
            "cell_type": "code", 
            "execution_count": null, 
            "source": "##\n## BOOTSTRAP/BOOSTING TRAINING WITH LOO\n##\n\nprint \"Training\",\npredictions = np.array([]).reshape(0,2)\nbstrapTrainingSet = training_set\nthreshold = .0\n_, test_y = split(test_set, NUM_FEATURES)\ninitialTestValue = 394\n\nbstrapTrainingSet = bstrapTrainingSet.append(pandas.DataFrame(test_set.values[:initialTestValue,:]))\n#print bstrapTrainingSet\n\ntry:\n    for i in range(initialTestValue,len(test_set),2):\n\n        test_rows = pandas.DataFrame(test_set.values[[i, i+1],:])\n        success = False\n        retry = 0\n        while ((~success) & (retry<5)):\n            try:\n                ## CHOOSE BOOTSTRAP OR BOOST\n                results = boostingTrain(bstrapTrainingSet, test_rows, .01, 20, False)\n                #results = bootstrapTrain(bstrapTrainingSet, test_rows, .01, 20, threshold, False)\n                predictions =  np.concatenate([predictions, np.nanmean(results[\"test_predictions\"], axis=0)])    \n                success = True\n            except ValueError:  \n                log.emit_log( {'app_name': 'Experiment2','type': 'error','message': \"ValueError - Retrying...\"})\n                retry = retry + 1\n                \n            \n        bstrapTrainingSet = bstrapTrainingSet.append(test_rows)\n        # Window\n        bstrapTrainingSet = bstrapTrainingSet[-len(training_set):]\n\n        res = evaluate(predictions, test_y[initialTestValue:initialTestValue+len(predictions),:], threshold)\n        msg = str(\"Results after %d iterations, %.2f precision, %.2f recall at %.2f threshold\" % (i+2, res[0], res[1], threshold))\n        print \".\"\n        print msg\n\n        log.emit_log( {'app_name': 'Experiment2','type': 'result','message': msg})\n        # 15/02/18 - Bluemix no longer using logmet for metrics\n        #metrics.emit_metric(name='Experiment2.precision', value=res[0])\n        #metrics.emit_metric(name='Experiment2.recall', value=res[1])\n\n        pandas.DataFrame(predictions).to_csv(\"results_new2.csv\", header=False, index=False)\n        put_file('Experiment2', \"results_new2.csv\")\n\n        # Try to free memory\n        gc.collect()\nexcept:\n    print(\"Unexpected error: %s\" % sys.exc_info()[0])\n    log.emit_log( {'app_name': 'Experiment2','type': 'error','message': str(\"Unexpected error: %s\" % sys.exc_info()[0])})\n    raise\n    "
        }, 
        {
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Iteration : 19 Lambda : 0.10, Threshold : 0.00\nTraining loss : 0.61+/-0.02, precision : 0.66+/-0.02, recall : 1.00+/-0.00, F : 0.79+/-0.02\nValidation loss : 0.72+/-0.02, precision : 0.54+/-0.02, recall : 1.00+/-0.00, F : 0.70+/-0.01\nTest loss : 0.73+/-0.02, precision : 0.54+/-0.02, recall : 1.00+/-0.00, F : 0.70+/-0.02\n", 
                    "name": "stdout"
                }, 
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "execution_count": 11, 
                    "data": {
                        "text/plain": "(0.54883718, 1.0, 0.70870868836909162, array([[ 0.56349051,  0.43598217],\n        [ 0.55803794,  0.44130236],\n        [ 0.5493713 ,  0.45034656],\n        [ 0.55059552,  0.4489843 ],\n        [ 0.49326783,  0.50540805],\n        [ 0.45126668,  0.54793656],\n        [ 0.65567821,  0.34172443],\n        [ 0.60610175,  0.39114138],\n        [ 0.48215455,  0.51353896],\n        [ 0.46498594,  0.53102314],\n        [ 0.37359807,  0.62640458],\n        [ 0.42384404,  0.57626975],\n        [ 0.5093075 ,  0.49005389],\n        [ 0.55047226,  0.44859108],\n        [ 0.49225932,  0.50419605],\n        [ 0.51646715,  0.47996092],\n        [ 0.79734296,  0.20354548],\n        [ 0.62736219,  0.3730047 ],\n        [ 0.6468395 ,  0.35361773],\n        [ 0.62354386,  0.37692541],\n        [ 0.49336094,  0.50491095],\n        [ 0.56665027,  0.43155614],\n        [ 0.59447235,  0.40554476],\n        [ 0.68440908,  0.31626263],\n        [ 0.60287625,  0.39587271],\n        [ 0.35190681,  0.64677334],\n        [ 0.37363994,  0.62492323],\n        [ 0.53190124,  0.46834469],\n        [ 0.56040841,  0.43985391],\n        [ 0.4381001 ,  0.56178331],\n        [ 0.60314953,  0.39699331],\n        [ 0.44454059,  0.55529654],\n        [ 0.45496202,  0.54496253],\n        [ 0.48666853,  0.51060677],\n        [ 0.47532672,  0.52287948],\n        [ 0.59239781,  0.40752333],\n        [ 0.64675081,  0.35339308],\n        [ 0.7668643 ,  0.23319077],\n        [ 0.82283628,  0.17714676],\n        [ 0.69956267,  0.30022103],\n        [ 0.5798772 ,  0.4202489 ],\n        [ 0.66586792,  0.33432162],\n        [ 0.67764986,  0.32253858],\n        [ 0.60325444,  0.39663318],\n        [ 0.61292666,  0.38708395],\n        [ 0.54231304,  0.45781499],\n        [ 0.52552736,  0.47447667],\n        [ 0.68160552,  0.31895483],\n        [ 0.71362698,  0.28705853],\n        [ 0.48949033,  0.50713235],\n        [ 0.74978209,  0.24987483],\n        [ 0.79754418,  0.20258348],\n        [ 0.43083686,  0.5670982 ],\n        [ 0.60187954,  0.39558965],\n        [ 0.57497877,  0.42425117],\n        [ 0.55728018,  0.44131646],\n        [ 0.60466999,  0.39598161],\n        [ 0.46573567,  0.53489262],\n        [ 0.53068548,  0.46726584],\n        [ 0.45257378,  0.5459435 ],\n        [ 0.56866264,  0.43051353],\n        [ 0.57010752,  0.42899504],\n        [ 0.53537494,  0.46531922],\n        [ 0.44112229,  0.55937654],\n        [ 0.5673449 ,  0.43258128],\n        [ 0.56417328,  0.43558717],\n        [ 0.56051767,  0.43941212],\n        [ 0.49838576,  0.50132006],\n        [ 0.34718838,  0.65233493],\n        [ 0.34194043,  0.65741342],\n        [ 0.73271692,  0.26739058],\n        [ 0.67960304,  0.32063875],\n        [ 0.64777613,  0.35231584],\n        [ 0.63695782,  0.363159  ],\n        [ 0.66103965,  0.33871803],\n        [ 0.60738492,  0.39250535],\n        [ 0.67117423,  0.32904148],\n        [ 0.63778722,  0.36235639],\n        [ 0.39724785,  0.59751523],\n        [ 0.39023483,  0.60435647],\n        [ 0.50698173,  0.48972359],\n        [ 0.56628489,  0.43197599],\n        [ 0.63324535,  0.36594898],\n        [ 0.52857792,  0.47074217],\n        [ 0.45921469,  0.53715098],\n        [ 0.45755357,  0.5387013 ],\n        [ 0.37581277,  0.62354213],\n        [ 0.41259518,  0.58702308],\n        [ 0.63714272,  0.36244059],\n        [ 0.64189529,  0.35774589],\n        [ 0.47846174,  0.52280974],\n        [ 0.53273201,  0.46855631],\n        [ 0.42773151,  0.57190788],\n        [ 0.42210108,  0.57751387],\n        [ 0.61912483,  0.3811999 ],\n        [ 0.63745558,  0.36309895],\n        [ 0.62861979,  0.36975104],\n        [ 0.61473233,  0.38359779],\n        [ 0.46714824,  0.52941817],\n        [ 0.44168973,  0.55423933],\n        [ 0.49085122,  0.50916272],\n        [ 0.46857166,  0.53135878],\n        [ 0.50646245,  0.49115077],\n        [ 0.50488079,  0.49266329],\n        [ 0.81380051,  0.18705064],\n        [ 0.42665347,  0.57073259],\n        [ 0.44765848,  0.54927647],\n        [ 0.46124762,  0.53840697],\n        [ 0.44423485,  0.55513537],\n        [ 0.71555877,  0.28514582],\n        [ 0.74338877,  0.25711909],\n        [ 0.50101161,  0.49726361],\n        [ 0.50138748,  0.49716097],\n        [ 0.56440985,  0.4327454 ],\n        [ 0.60787189,  0.38994634],\n        [ 0.5824461 ,  0.41674528],\n        [ 0.53413385,  0.46560127],\n        [ 0.6661402 ,  0.3338972 ],\n        [ 0.6019336 ,  0.39793986],\n        [ 0.49942812,  0.50088519],\n        [ 0.40844679,  0.59151495],\n        [ 0.56537259,  0.43346101],\n        [ 0.37731823,  0.62243044],\n        [ 0.67996109,  0.31958026],\n        [ 0.75110209,  0.24957797],\n        [ 0.68939793,  0.31102341],\n        [ 0.61926889,  0.38098684],\n        [ 0.69114697,  0.30978927],\n        [ 0.65918428,  0.34167647],\n        [ 0.45189095,  0.54774123],\n        [ 0.47096148,  0.52877784],\n        [ 0.65242726,  0.34848207],\n        [ 0.68836153,  0.3124736 ],\n        [ 0.6171003 ,  0.38272029],\n        [ 0.61694771,  0.38289005],\n        [ 0.69897163,  0.30235535],\n        [ 0.68724823,  0.31399077],\n        [ 0.50221771,  0.49384624],\n        [ 0.5168072 ,  0.47866336],\n        [ 0.63553417,  0.36386368],\n        [ 0.54340369,  0.45637351],\n        [ 0.68333805,  0.31755176],\n        [ 0.71222746,  0.28898883],\n        [ 0.56859481,  0.42879978],\n        [ 0.50677067,  0.4916535 ],\n        [ 0.55916685,  0.44012523],\n        [ 0.49517098,  0.50457072],\n        [ 0.40525398,  0.59296477],\n        [ 0.54102826,  0.45851183],\n        [ 0.66937071,  0.33126751],\n        [ 0.60322779,  0.39756528],\n        [ 0.37594101,  0.62102532],\n        [ 0.35051581,  0.64721376],\n        [ 0.65089595,  0.3499079 ],\n        [ 0.74548292,  0.25519371],\n        [ 0.29915825,  0.69994092],\n        [ 0.28236932,  0.71579146],\n        [ 0.53741711,  0.46254927],\n        [ 0.5171597 ,  0.48251885],\n        [ 0.28924301,  0.71018058],\n        [ 0.3723557 ,  0.62733829],\n        [ 0.68839633,  0.31170946],\n        [ 0.55961359,  0.44025463],\n        [ 0.67035669,  0.32801095],\n        [ 0.67134011,  0.32792339],\n        [ 0.51318967,  0.48674124],\n        [ 0.53450531,  0.46583337],\n        [ 0.553774  ,  0.44622749],\n        [ 0.59329164,  0.40709901],\n        [ 0.53638518,  0.46362215],\n        [ 0.54413384,  0.45590544],\n        [ 0.46213251,  0.53587663],\n        [ 0.5259701 ,  0.47279748],\n        [ 0.59027147,  0.41034192],\n        [ 0.58115447,  0.41939536],\n        [ 0.58516896,  0.41589522],\n        [ 0.59319633,  0.40769738],\n        [ 0.34534478,  0.65272987],\n        [ 0.30583924,  0.69228184],\n        [ 0.51300251,  0.48543635],\n        [ 0.45342818,  0.54514062],\n        [ 0.53064507,  0.46757206],\n        [ 0.59732753,  0.40189236],\n        [ 0.45743114,  0.54240417],\n        [ 0.28466213,  0.71414739],\n        [ 0.63561761,  0.36518651],\n        [ 0.69407463,  0.30671138],\n        [ 0.71692765,  0.28318462],\n        [ 0.59187084,  0.40854368],\n        [ 0.54842472,  0.45007071],\n        [ 0.57539207,  0.42492366],\n        [ 0.49844417,  0.50013989],\n        [ 0.73753309,  0.263008  ],\n        [ 0.55233783,  0.4468399 ],\n        [ 0.65867102,  0.34162816],\n        [ 0.6506924 ,  0.3495093 ],\n        [ 0.75453031,  0.24519622],\n        [ 0.5301978 ,  0.47009364],\n        [ 0.542817  ,  0.45734635],\n        [ 0.44762579,  0.54876179],\n        [ 0.45095968,  0.54399371],\n        [ 0.51856393,  0.48066568],\n        [ 0.56638986,  0.43291169],\n        [ 0.60101444,  0.39915532],\n        [ 0.59046084,  0.40948582],\n        [ 0.65834463,  0.34006542],\n        [ 0.47755843,  0.52085072],\n        [ 0.68399704,  0.31692678],\n        [ 0.59190041,  0.40887338],\n        [ 0.52999079,  0.47047171],\n        [ 0.61097604,  0.38962007],\n        [ 0.48100781,  0.51722246],\n        [ 0.48527345,  0.51246226],\n        [ 0.55963475,  0.44042307],\n        [ 0.67621744,  0.32328016],\n        [ 0.56071705,  0.44066143],\n        [ 0.51329905,  0.4879787 ],\n        [ 0.28677934,  0.71392637],\n        [ 0.32119057,  0.67933083],\n        [ 0.54300493,  0.45702633],\n        [ 0.47317871,  0.52621847],\n        [ 0.51401263,  0.48322755],\n        [ 0.55361903,  0.44393414],\n        [ 0.53321326,  0.46759909],\n        [ 0.43433553,  0.56563747],\n        [ 0.58110094,  0.41953006],\n        [ 0.62935269,  0.37107977],\n        [ 0.55585128,  0.44040671],\n        [ 0.46825185,  0.52733898],\n        [ 0.70564669,  0.29448938],\n        [ 0.55846769,  0.44197845],\n        [ 0.51160252,  0.48831731],\n        [ 0.49882931,  0.50051618],\n        [ 0.60336143,  0.39708591],\n        [ 0.62013584,  0.38071519],\n        [ 0.53191686,  0.46784028],\n        [ 0.54242432,  0.45741791],\n        [ 0.47566819,  0.52350682],\n        [ 0.45090857,  0.54745305],\n        [ 0.57625467,  0.42412257],\n        [ 0.55223542,  0.44799748],\n        [ 0.49336511,  0.50205642],\n        [ 0.57640868,  0.41980043],\n        [ 0.46262449,  0.53606582],\n        [ 0.46401659,  0.53383571],\n        [ 0.63931841,  0.36022109],\n        [ 0.67162758,  0.32912153],\n        [ 0.7477479 ,  0.25253576],\n        [ 0.70947617,  0.29105359],\n        [ 0.59461957,  0.40527377],\n        [ 0.5320307 ,  0.46762285],\n        [ 0.45047134,  0.54891962],\n        [ 0.56197697,  0.43780333],\n        [ 0.62836778,  0.3710191 ],\n        [ 0.58045638,  0.41822115],\n        [ 0.3395223 ,  0.65909255],\n        [ 0.42990202,  0.56904668],\n        [ 0.73405492,  0.26605687],\n        [ 0.71590829,  0.28388944],\n        [ 0.50843394,  0.49197865],\n        [ 0.53421897,  0.46618944],\n        [ 0.60174572,  0.39682627],\n        [ 0.57906961,  0.41936746],\n        [ 0.54583734,  0.45447105],\n        [ 0.5632531 ,  0.43704548],\n        [ 0.6279465 ,  0.37267029],\n        [ 0.38592595,  0.61365449],\n        [ 0.35429388,  0.64516938],\n        [ 0.41458598,  0.58567423],\n        [ 0.42863148,  0.57157469],\n        [ 0.45013767,  0.55036271],\n        [ 0.48394832,  0.51669264],\n        [ 0.56436783,  0.4368422 ],\n        [ 0.47266045,  0.52602208],\n        [ 0.46860391,  0.52894545],\n        [ 0.60707378,  0.39149183],\n        [ 0.59307712,  0.40475386],\n        [ 0.54898185,  0.44736147],\n        [ 0.48801059,  0.50740576],\n        [ 0.47138205,  0.52407181],\n        [ 0.4776248 ,  0.51742852],\n        [ 0.64614981,  0.35433966],\n        [ 0.59690017,  0.40308747],\n        [ 0.47402558,  0.52553302],\n        [ 0.46071452,  0.53860378],\n        [ 0.56991363,  0.42707771],\n        [ 0.53913546,  0.45743698],\n        [ 0.61557364,  0.38360995],\n        [ 0.59147054,  0.40792933],\n        [ 0.49775761,  0.50251532],\n        [ 0.63279188,  0.36689806],\n        [ 0.44822326,  0.55209148],\n        [ 0.46606153,  0.5345149 ],\n        [ 0.54856437,  0.45115834],\n        [ 0.5070529 ,  0.49303871],\n        [ 0.69283044,  0.30841336],\n        [ 0.71607989,  0.28526625],\n        [ 0.75646168,  0.2442157 ],\n        [ 0.71959174,  0.28156179],\n        [ 0.63937026,  0.36145684],\n        [ 0.63245255,  0.36834517],\n        [ 0.5682807 ,  0.43111163],\n        [ 0.43718162,  0.56206703],\n        [ 0.60857391,  0.3889913 ],\n        [ 0.76567566,  0.23366162],\n        [ 0.50253761,  0.49463564],\n        [ 0.47956786,  0.518305  ],\n        [ 0.8368004 ,  0.16312653],\n        [ 0.6766504 ,  0.32401189],\n        [ 0.68783861,  0.31275064],\n        [ 0.47078162,  0.5277524 ],\n        [ 0.46351686,  0.5340749 ],\n        [ 0.44107738,  0.55813074],\n        [ 0.56720084,  0.43183333],\n        [ 0.48457879,  0.51531708],\n        [ 0.43796173,  0.56182837],\n        [ 0.52048004,  0.47990996],\n        [ 0.56762135,  0.43279251],\n        [ 0.53227109,  0.46455398],\n        [ 0.47314644,  0.52437031],\n        [ 0.59388053,  0.40465093],\n        [ 0.64293474,  0.35647774],\n        [ 0.61755437,  0.38147321],\n        [ 0.57410902,  0.42485422],\n        [ 0.57716298,  0.42372495],\n        [ 0.52798963,  0.47283712],\n        [ 0.56756872,  0.42973199],\n        [ 0.62743843,  0.36947539],\n        [ 0.59846377,  0.40225673],\n        [ 0.62421441,  0.37644666],\n        [ 0.51580733,  0.48415518],\n        [ 0.50904042,  0.49084902],\n        [ 0.59903371,  0.40007505],\n        [ 0.57609421,  0.42287144],\n        [ 0.51627362,  0.4823814 ],\n        [ 0.41574207,  0.5802151 ],\n        [ 0.73590815,  0.26459348],\n        [ 0.76467258,  0.23510173],\n        [ 0.57779306,  0.41997319],\n        [ 0.61197686,  0.38526338],\n        [ 0.51250088,  0.48747236],\n        [ 0.55625582,  0.44398522],\n        [ 0.66786587,  0.33307016],\n        [ 0.60188711,  0.3990151 ],\n        [ 0.3915222 ,  0.60787356],\n        [ 0.42131978,  0.57807767],\n        [ 0.48955497,  0.50881815],\n        [ 0.417317  ,  0.58010668],\n        [ 0.47766656,  0.5182597 ],\n        [ 0.45267239,  0.54282802],\n        [ 0.36089164,  0.63833088],\n        [ 0.4197982 ,  0.57922655],\n        [ 0.51565802,  0.48442894],\n        [ 0.50782299,  0.49223408],\n        [ 0.34139255,  0.65781707],\n        [ 0.33592883,  0.6623711 ],\n        [ 0.61873144,  0.38190421],\n        [ 0.63039374,  0.37026295],\n        [ 0.42209798,  0.57789314],\n        [ 0.510225  ,  0.48989087],\n        [ 0.6425522 ,  0.35818106],\n        [ 0.72399545,  0.27678156],\n        [ 0.60033917,  0.40014252],\n        [ 0.60085601,  0.39972079],\n        [ 0.48195568,  0.51490039],\n        [ 0.43205777,  0.56570244],\n        [ 0.58686733,  0.41352138],\n        [ 0.63424087,  0.36601844],\n        [ 0.53492922,  0.46549225],\n        [ 0.51198781,  0.48839736],\n        [ 0.60543698,  0.39576173],\n        [ 0.58524972,  0.41585541],\n        [ 0.69304281,  0.30810043],\n        [ 0.5473994 ,  0.45342809],\n        [ 0.53227872,  0.46791488],\n        [ 0.61773986,  0.38282233],\n        [ 0.64929283,  0.3506636 ],\n        [ 0.62864184,  0.37057692],\n        [ 0.45413595,  0.54608762],\n        [ 0.4605813 ,  0.53961837],\n        [ 0.47020221,  0.52512008],\n        [ 0.50508893,  0.4909336 ],\n        [ 0.69525778,  0.30521259],\n        [ 0.69911999,  0.30164301],\n        [ 0.60838401,  0.39187747],\n        [ 0.61214441,  0.38804823],\n        [ 0.4357006 ,  0.5640384 ],\n        [ 0.4056274 ,  0.59398508],\n        [ 0.5428229 ,  0.45782328],\n        [ 0.55147147,  0.44888002],\n        [ 0.6562137 ,  0.34330767],\n        [ 0.61180723,  0.38757047],\n        [ 0.56403804,  0.43675485],\n        [ 0.61162651,  0.38872641],\n        [ 0.69127953,  0.30937091],\n        [ 0.6508497 ,  0.35000309],\n        [ 0.4551689 ,  0.54454869],\n        [ 0.45332795,  0.5460009 ],\n        [ 0.43084654,  0.56721544],\n        [ 0.43884   ,  0.55989152],\n        [ 0.60193408,  0.39772898],\n        [ 0.55028683,  0.44942427],\n        [ 0.56211305,  0.43820673],\n        [ 0.50268573,  0.49765295],\n        [ 0.69801801,  0.30245262],\n        [ 0.54645073,  0.45368189],\n        [ 0.50802934,  0.49012899],\n        [ 0.43766189,  0.55954754],\n        [ 0.72981346,  0.27060255],\n        [ 0.76045954,  0.23973426],\n        [ 0.61629266,  0.38378358],\n        [ 0.58619386,  0.41338927],\n        [ 0.68384135,  0.31656164],\n        [ 0.484438  ,  0.51561987],\n        [ 0.73265588,  0.26680726],\n        [ 0.63487005,  0.36439055],\n        [ 0.60640943,  0.39394444],\n        [ 0.74921447,  0.25078014],\n        [ 0.560085  ,  0.44054455],\n        [ 0.54194057,  0.45859247],\n        [ 0.61631167,  0.38297975],\n        [ 0.51066601,  0.48889661],\n        [ 0.63045335,  0.37049347],\n        [ 0.58907712,  0.41157594],\n        [ 0.50552243,  0.49398232],\n        [ 0.51411867,  0.48537365],\n        [ 0.39050871,  0.60927045],\n        [ 0.38463581,  0.61517668],\n        [ 0.57231343,  0.4276436 ],\n        [ 0.52495223,  0.47443604]], dtype=float32))"
                    }
                }
            ], 
            "cell_type": "code", 
            "execution_count": 11, 
            "source": "##\n## BOOTSTRAP TRAINING\n##\n\nprint \"Training\",\n_, test_y = split(test_set, NUM_FEATURES)\nresults = bootstrapTrain(training_set, test_set, .1, 20, .0, True)\npredictions2 =  np.nanmean(results[\"test_predictions\"], axis=0)\nevaluate(predictions2, test_y, .5)"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "source": ""
        }
    ], 
    "nbformat_minor": 2
}