{
    "nbformat_minor": 2, 
    "cells": [
        {
            "source": "## Model : marketdirection\n### Description :\nThis model uses a Tensorflow neural network to predict the direction of a market in the next Y periods, based on the values of the previous X periods. \n\n### Model Attributes :\n- FFNN\n- Boosting\n- Re-training of entire network for each additional period\n\n### USP :\n- Normalised market data (between 0 and 1) to highlight common patterns at any time scale.\n- Utilises similar markets to increase size of training set\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#\n# Get dataset from MI API #\n#\n\nimport pandas\n\n!pip install git+https://github.com/cwilko/quantutils.git\nimport quantutils.dataset.pipeline as ppl\nfrom quantutils.cloud.bluemix import ObjectStore, MarketInsights\n\nmi = MarketInsights('cred/MIOapi_cred.json')\n\ndow = mi.jsontocsv(mi.get_dataset(\"DOW\", \"marketdirection\"))\nspy = mi.jsontocsv(mi.get_dataset(\"SPY\", \"marketdirection\"))\n\n# Interleave (part of the \"added insight\" for this model)\ndataset = pandas.concat([spy,dow]).sort_index().reset_index(drop=False)\n", 
            "cell_type": "code", 
            "execution_count": 20, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "# LEGACY - Load from objectstore #\nimport pandas \nimport json\nimport gc\nimport sys\nfrom bluemix import *\n\nobjStore = ObjectStore('object_storage_cred.json')\n#logging_cred = json.load(open('logging_cred.json'))\n#log = get_logging_client(logging_cred)\n\ndataset = pandas.read_csv(objStore.get_file('Experiment2', 'Experiment2_zero.csv'), header=None)\n", 
            "cell_type": "code", 
            "execution_count": 36, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "testSetLength = 430\ntraining_set = dataset[:-(testSetLength)]\ntest_set = dataset[-(testSetLength):]", 
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\n##### Specific to the data ##\nNUM_FEATURES = (2 * 4) + 1\nNUM_LABELS = 2\n#############################\n\n\n##### #Define the architecture\nHIDDEN_UNITS = 32\n\n# The random seed that defines initialization.\nSEED = 42\n\n# This is where training samples and labels are fed to the graph.\n# These placeholder nodes will be fed a batch of training data at each\n# training step, which we'll write once we define the graph structure.\ntrain_data_node = tf.placeholder(tf.float32, shape=(None, NUM_FEATURES))\ntrain_labels_node = tf.placeholder(tf.float32, shape=(None, NUM_LABELS))\nlam = tf.placeholder(tf.float32)\n\n# The variables below hold all the trainable weights. For each, the\n# parameter defines how the variables will be initialized. \n# TODO : These should be pulled from a config file\n\nTheta1 = tf.Variable( tf.truncated_normal([HIDDEN_UNITS, (NUM_FEATURES)], stddev=0.1))\n\nTheta2 = tf.Variable( tf.truncated_normal([NUM_LABELS, HIDDEN_UNITS],stddev=0.1))\nbias2 = tf.Variable(tf.constant(0.1, shape=[NUM_LABELS]))\n    \nprint('Done')", 
            "cell_type": "code", 
            "execution_count": 4, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Done\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "def model(X, Theta1, Theta2, bias):\n    \"\"\"The Model definition.\"\"\"\n    # Perceptron\n    \n    layer1 = tf.nn.sigmoid(tf.matmul(X, tf.transpose(Theta1)))\n                        \n    output = tf.nn.bias_add(tf.matmul(layer1, tf.transpose(Theta2)),bias)\n\n    return output\n    \nprint('Done')", 
            "cell_type": "code", 
            "execution_count": 5, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Done\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "yhat = model(train_data_node, Theta1, Theta2, bias2)\n\n# Change the weights by subtracting derivative with respect to that weight\nloss =  tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=train_labels_node, logits=yhat))\n# Regularization using L2 Loss function \nregularizer = tf.nn.l2_loss(Theta1) + tf.nn.l2_loss(Theta2)\nreg = (lam / tf.to_float(tf.shape(train_labels_node)[0])) * regularizer\nloss_reg = loss + reg\n\n# Optimizer: \n\n# Gradient Descent\noptimizer = tf.contrib.opt.ScipyOptimizerInterface(loss_reg, options={'maxiter':4000})\n#update_weights = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n\n# Predictions\ntrain_prediction = tf.sigmoid(yhat)\n\nprint('Done')", 
            "cell_type": "code", 
            "execution_count": 6, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Done\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "# Keep track of the loss at each iteration so we can chart it later\nJ = []\n\ndef loss_callback():\n    print \"Recalc...\"\n    \ndef step_callback(params):\n    J.append(params)\n    \ndef split(data, num_features):    \n    return data.values[:,:num_features], data.values[:,num_features:]\n    \ndef sample(training_set, method=\"RANDOM\", prop=.9, loo=0, boost = []): \n    if (method == \"RANDOM\"):\n        training_set = training_set.sample(frac=1).reset_index(drop=True)\n        idx = np.arange(0,len(training_set)) / float(len(training_set))\n        return [training_set[idx<prop], training_set[idx>=prop]]\n    elif (method == \"LOO\"):\n        idx = np.array(range(0,len(training_set)))\n        return [training_set[idx!=loo], training_set[idx==loo]]\n    elif (method == \"BOOTSTRAP\"):\n        idx = np.array(range(0,len(training_set)))\n        sample = np.random.choice(idx, len(training_set), replace=True)\n        return pandas.DataFrame(training_set.values[sample,:]), training_set[~np.in1d(idx, sample)]\n    elif (method == \"BOOSTING\"):\n        idx = np.array(range(0,len(training_set)))\n        sample = np.random.choice(idx, len(training_set), replace=True, p=boost)\n        return pandas.DataFrame(training_set.values[sample,:]), training_set[~np.in1d(idx, sample)]\n    \ndef minimize(feed_dict, train=True):\n    \n    #optimizer.minimize(feed_dict=feed_dict, fetches=[loss_reg], loss_callback=loss_callback)\n    if (train):\n        optimizer.minimize(feed_dict=feed_dict)\n\n    return loss.eval(feed_dict), train_prediction.eval(feed_dict)\n\ndef evaluate(predictions, data_y, threshold):\n    a = np.argmax(predictions,axis=1) \n    b = np.argmax(data_y,axis=1) \n    a = a[(predictions > threshold).any(axis=1)]\n    b = b[(predictions > threshold).any(axis=1)]\n    precision = np.float32(np.sum(a == b) / np.float32(b.shape[0]))\n    recall = np.float32(np.sum(a == b) / np.float32(data_y.shape[0])) # Correct Recall\n    recall = np.float32(b.shape[0]) / data_y.shape[0] # Number of Days traded\n    F_score = (2.0 * precision * recall) / (precision + recall)\n    return precision, recall, F_score, predictions\n\ndef predict(data_X, data_y, lam1, threshold):    \n    loss, predictions = minimize({train_data_node: data_X, train_labels_node: data_y, lam: lam1}, train=False)\n    precision, recall, F_score, predictions = evaluate(predictions, data_y, threshold)\n    return loss, precision, recall, F_score, predictions\n    \n\ndef train(train_dict, val_dict, test_dict, threshold, iterations=50, debug=True):\n    \n    tf.logging.set_verbosity(tf.logging.ERROR)\n    \n    metrics = {\n        \"train_loss\":[],\n        \"train_precision\":[],\n        \"train_recall\":[],\n        \"train_f\":[],\n        \"val_loss\":[],\n        \"val_precision\":[],\n        \"val_recall\":[],\n        \"val_f\":[],\n        \"test_loss\":[],\n        \"test_precision\":[],\n        \"test_recall\":[],\n        \"test_f\":[],\n        \"test_predictions\":[]\n    }\n    \n    for i in range(0,iterations):\n        \n        for j in range(0, 50):\n            \n            # Create a new interactive session that we'll use in\n            # subsequent code cells.\n            s = tf.InteractiveSession()\n            s.as_default()\n\n            # Initialize all the variables we defined above.\n            tf.initialize_all_variables().run()\n\n            minimize(train_dict)\n            train_loss, train_precision, train_recall, train_f, _ = predict(train_dict[train_data_node], train_dict[train_labels_node], train_dict[lam], threshold)\n\n            if (train_loss < .65):\n                print \".\",\n                metrics[\"train_loss\"].append(train_loss)\n                metrics[\"train_precision\"].append(train_precision)\n                metrics[\"train_recall\"].append(train_recall)\n                metrics[\"train_f\"].append(train_f)\n\n                val_loss, val_precision, val_recall, val_f, _= predict(val_dict[train_data_node], val_dict[train_labels_node], val_dict[lam], threshold)\n\n                metrics[\"val_loss\"].append(val_loss)\n                metrics[\"val_precision\"].append(val_precision)\n                metrics[\"val_recall\"].append(val_recall)\n                metrics[\"val_f\"].append(val_f)\n                \n                test_loss, test_precision, test_recall, test_f, test_predictions = predict(test_dict[train_data_node], test_dict[train_labels_node], test_dict[lam], threshold)\n\n                metrics[\"test_loss\"].append(test_loss)\n                metrics[\"test_precision\"].append(test_precision)\n                metrics[\"test_recall\"].append(test_recall)\n                metrics[\"test_f\"].append(test_f)\n                metrics[\"test_predictions\"] = test_predictions # Return the last set of predictions (could return the one with the best val score)\n                del s\n                break;\n            else:\n                del s\n        \n        if (j >= 50):\n            print(\"ERROR : Failed to minimise function\")\n            \n    results = {\n        \"train_loss\": {\"mean\":np.nanmean(metrics[\"train_loss\"]), \"std\":np.nanstd(metrics[\"train_loss\"]), \"values\":metrics[\"train_loss\"]},\n        \"train_precision\": {\"mean\":np.nanmean(metrics[\"train_precision\"]), \"std\":np.nanstd(metrics[\"train_precision\"]), \"values\":metrics[\"train_precision\"]},\n        \"train_recall\": {\"mean\":np.nanmean(metrics[\"train_recall\"]), \"std\":np.nanstd(metrics[\"train_recall\"]), \"values\":metrics[\"train_recall\"]},\n        \"train_f\": {\"mean\":np.nanmean(metrics[\"train_f\"]), \"std\":np.nanstd(metrics[\"train_f\"]), \"values\":metrics[\"train_f\"]},\n        \"val_loss\": {\"mean\":np.nanmean(metrics[\"val_loss\"]), \"std\":np.nanstd(metrics[\"val_loss\"]), \"values\":metrics[\"val_loss\"]},\n        \"val_precision\":{\"mean\":np.nanmean(metrics[\"val_precision\"]), \"std\":np.nanstd(metrics[\"val_precision\"]), \"values\":metrics[\"val_precision\"]},\n        \"val_recall\": {\"mean\":np.nanmean(metrics[\"val_recall\"]), \"std\":np.nanstd(metrics[\"val_recall\"]), \"values\":metrics[\"val_recall\"]},\n        \"val_f\": {\"mean\":np.nanmean(metrics[\"val_f\"]), \"std\":np.nanstd(metrics[\"val_f\"]), \"values\":metrics[\"val_f\"]},\n        \"test_loss\": {\"mean\":np.nanmean(metrics[\"test_loss\"]), \"std\":np.nanstd(metrics[\"test_loss\"]), \"values\":metrics[\"test_loss\"]},\n        \"test_precision\":{\"mean\":np.nanmean(metrics[\"test_precision\"]), \"std\":np.nanstd(metrics[\"test_precision\"]), \"values\":metrics[\"test_precision\"]},\n        \"test_recall\": {\"mean\":np.nanmean(metrics[\"test_recall\"]), \"std\":np.nanstd(metrics[\"test_recall\"]), \"values\":metrics[\"test_recall\"]},\n        \"test_f\": {\"mean\":np.nanmean(metrics[\"test_f\"]), \"std\":np.nanstd(metrics[\"test_f\"]), \"values\":metrics[\"test_f\"]},\n        \"test_predictions\": metrics[\"test_predictions\"],\n    }\n    \n    print \".\",\n    if debug:\n        print(\"Iterations : %d Lambda : %.2f, Threshold : %.2f\" % (iterations, val_dict[lam], threshold))\n        print(\"Training loss : %.2f+/-%.2f, precision : %.2f+/-%.2f, recall : %.2f+/-%.2f, F : %.2f+/-%.2f\" % \n              (results[\"train_loss\"][\"mean\"], results[\"train_loss\"][\"std\"],\n               results[\"train_precision\"][\"mean\"], results[\"train_precision\"][\"std\"],\n               results[\"train_recall\"][\"mean\"], results[\"train_recall\"][\"std\"],\n               results[\"train_f\"][\"mean\"], results[\"train_f\"][\"std\"]))\n        print(\"Validation loss : %.2f+/-%.2f, precision : %.2f+/-%.2f, recall : %.2f+/-%.2f, F : %.2f+/-%.2f\" % \n              (results[\"val_loss\"][\"mean\"], results[\"val_loss\"][\"std\"],\n               results[\"val_precision\"][\"mean\"], results[\"val_precision\"][\"std\"],\n               results[\"val_recall\"][\"mean\"], results[\"val_recall\"][\"std\"],\n               results[\"val_f\"][\"mean\"], results[\"val_f\"][\"std\"]))\n        print(\"Test loss : %.2f+/-%.2f, precision : %.2f+/-%.2f, recall : %.2f+/-%.2f, F : %.2f+/-%.2f\" % \n              (results[\"test_loss\"][\"mean\"], results[\"test_loss\"][\"std\"],\n               results[\"test_precision\"][\"mean\"], results[\"test_precision\"][\"std\"],\n               results[\"test_recall\"][\"mean\"], results[\"test_recall\"][\"std\"],\n               results[\"test_f\"][\"mean\"], results[\"test_f\"][\"std\"]))\n\n    return results", 
            "cell_type": "code", 
            "execution_count": 7, 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "### \n### CROSS-VAL RANDOM SAMPLING\n###\n\ndef bootstrapTrain(training_set, test_set, lamda, iterations, threshold, debug=False):\n\n    metrics = {\n        \"train_loss\":[],\n        \"train_precision\":[],\n        \"train_recall\":[],\n        \"train_f\":[],\n        \"val_loss\":[],\n        \"val_precision\":[],\n        \"val_recall\":[],\n        \"val_f\":[],\n        \"test_loss\":[],\n        \"test_precision\":[],\n        \"test_recall\":[],\n        \"test_f\":[],\n        \"test_predictions\":[]\n    }\n    \n    test_X, test_y = split(test_set, NUM_FEATURES)\n\n    for i in range(0, iterations):\n        \n        print \".\",\n\n        train_sample, val_sample = sample(training_set, method=\"BOOTSTRAP\", loo=i)\n\n        train_sample_X, train_sample_y = split(train_sample, NUM_FEATURES)\n        val_sample_X, val_sample_y = split(val_sample, NUM_FEATURES)        \n\n        results = train({train_data_node: train_sample_X, train_labels_node: train_sample_y, lam: lamda}, {train_data_node: val_sample_X, train_labels_node: val_sample_y, lam: lamda}, {train_data_node: test_X, train_labels_node: test_y, lam: lamda}, threshold, 1, False)\n\n        metrics[\"train_loss\"].append(results[\"train_loss\"][\"mean\"])\n        metrics[\"train_precision\"].append(results[\"train_precision\"][\"mean\"])\n        metrics[\"train_recall\"].append(results[\"train_recall\"][\"mean\"])\n        metrics[\"train_f\"].append(results[\"train_f\"][\"mean\"])\n        metrics[\"val_loss\"].append(results[\"val_loss\"][\"mean\"])\n        metrics[\"val_precision\"].append(results[\"val_precision\"][\"mean\"])\n        metrics[\"val_recall\"].append(results[\"val_recall\"][\"mean\"])\n        metrics[\"val_f\"].append(results[\"val_f\"][\"mean\"])\n        metrics[\"test_loss\"].append(results[\"test_loss\"][\"mean\"])\n        metrics[\"test_precision\"].append(results[\"test_precision\"][\"mean\"])\n        metrics[\"test_recall\"].append(results[\"test_recall\"][\"mean\"])\n        metrics[\"test_f\"].append(results[\"test_f\"][\"mean\"])\n        metrics[\"test_predictions\"].append(results[\"test_predictions\"])  \n\n\n    results = {\n        \"train_loss\": {\"mean\":np.nanmean(metrics[\"train_loss\"]), \"std\":np.nanstd(metrics[\"train_loss\"]), \"values\":metrics[\"train_loss\"]},\n        \"train_precision\": {\"mean\":np.nanmean(metrics[\"train_precision\"]), \"std\":np.nanstd(metrics[\"train_precision\"]), \"values\":metrics[\"train_precision\"]},\n        \"train_recall\": {\"mean\":np.nanmean(metrics[\"train_recall\"]), \"std\":np.nanstd(metrics[\"train_recall\"]), \"values\":metrics[\"train_recall\"]},\n        \"train_f\": {\"mean\":np.nanmean(metrics[\"train_f\"]), \"std\":np.nanstd(metrics[\"train_f\"]), \"values\":metrics[\"train_f\"]},\n        \"val_loss\": {\"mean\":np.nanmean(metrics[\"val_loss\"]), \"std\":np.nanstd(metrics[\"val_loss\"]), \"values\":metrics[\"val_loss\"]},\n        \"val_precision\":{\"mean\":np.nanmean(metrics[\"val_precision\"]), \"std\":np.nanstd(metrics[\"val_precision\"]), \"values\":metrics[\"val_precision\"]},\n        \"val_recall\": {\"mean\":np.nanmean(metrics[\"val_recall\"]), \"std\":np.nanstd(metrics[\"val_recall\"]), \"values\":metrics[\"val_recall\"]},\n        \"val_f\": {\"mean\":np.nanmean(metrics[\"val_f\"]), \"std\":np.nanstd(metrics[\"val_f\"]), \"values\":metrics[\"val_f\"]},\n        \"test_loss\": {\"mean\":np.nanmean(metrics[\"test_loss\"]), \"std\":np.nanstd(metrics[\"test_loss\"]), \"values\":metrics[\"test_loss\"]},\n        \"test_precision\":{\"mean\":np.nanmean(metrics[\"test_precision\"]), \"std\":np.nanstd(metrics[\"test_precision\"]), \"values\":metrics[\"test_precision\"]},\n        \"test_recall\": {\"mean\":np.nanmean(metrics[\"test_recall\"]), \"std\":np.nanstd(metrics[\"test_recall\"]), \"values\":metrics[\"test_recall\"]},\n        \"test_f\": {\"mean\":np.nanmean(metrics[\"test_f\"]), \"std\":np.nanstd(metrics[\"test_f\"]), \"values\":metrics[\"test_f\"]},\n        \"test_predictions\": metrics[\"test_predictions\"],\n    }\n\n    if debug:\n        print(\"Iteration : %d Lambda : %.2f, Threshold : %.2f\" % (i, lamda, threshold))\n        print(\"Training loss : %.2f+/-%.2f, precision : %.2f+/-%.2f, recall : %.2f+/-%.2f, F : %.2f+/-%.2f\" % \n              (results[\"train_loss\"][\"mean\"], results[\"train_loss\"][\"std\"],\n               results[\"train_precision\"][\"mean\"], results[\"train_precision\"][\"std\"],\n               results[\"train_recall\"][\"mean\"], results[\"train_recall\"][\"std\"],\n               results[\"train_f\"][\"mean\"], results[\"train_f\"][\"std\"]))\n        print(\"Validation loss : %.2f+/-%.2f, precision : %.2f+/-%.2f, recall : %.2f+/-%.2f, F : %.2f+/-%.2f\" % \n              (results[\"val_loss\"][\"mean\"], results[\"val_loss\"][\"std\"],\n               results[\"val_precision\"][\"mean\"], results[\"val_precision\"][\"std\"],\n               results[\"val_recall\"][\"mean\"], results[\"val_recall\"][\"std\"],\n               results[\"val_f\"][\"mean\"], results[\"val_f\"][\"std\"]))\n        print(\"Test loss : %.2f+/-%.2f, precision : %.2f+/-%.2f, recall : %.2f+/-%.2f, F : %.2f+/-%.2f\" % \n              (results[\"test_loss\"][\"mean\"], results[\"test_loss\"][\"std\"],\n               results[\"test_precision\"][\"mean\"], results[\"test_precision\"][\"std\"],\n               results[\"test_recall\"][\"mean\"], results[\"test_recall\"][\"std\"],\n               results[\"test_f\"][\"mean\"], results[\"test_f\"][\"std\"]))\n\n    return results\n", 
            "cell_type": "code", 
            "execution_count": 8, 
            "outputs": [], 
            "metadata": {
                "scrolled": false, 
                "collapsed": true
            }
        }, 
        {
            "source": "### \n### BOOSTING\n###\n\ndef boostingTrain(training_set, test_set, lamda, iterations, debug=False):\n\n    metrics = {\n        \"train_loss\":[],\n        \"train_precision\":[],\n        \"train_recall\":[],\n        \"train_f\":[],\n        \"val_loss\":[],\n        \"val_precision\":[],\n        \"val_recall\":[],\n        \"val_f\":[],\n        \"test_loss\":[],\n        \"test_precision\":[],\n        \"test_recall\":[],\n        \"test_f\":[],\n        \"test_predictions\":[]\n    }\n    \n    test_X, test_y = split(test_set, NUM_FEATURES)\n    train_X, train_y = split(training_set, NUM_FEATURES)\n    threshold = 0 # For boosting to work this must be 0\n    boost = np.array([1.0/len(training_set)] * len(training_set))\n\n    for i in range(0, iterations):\n        \n        print \".\",\n\n        train_sample, val_sample = sample(training_set, method=\"BOOSTING\", boost=boost)\n\n        train_sample_X, train_sample_y = split(train_sample, NUM_FEATURES)\n        val_sample_X, val_sample_y = split(val_sample, NUM_FEATURES)        \n\n        results = train({train_data_node: train_sample_X, train_labels_node: train_sample_y, lam: lamda}, {train_data_node: val_sample_X, train_labels_node: val_sample_y, lam: lamda}, {train_data_node: test_X, train_labels_node: test_y, lam: lamda}, threshold, 1, False)\n\n        #Evaluate the results and calculate the odds of misclassification\n        _, _, _, _, train_predictions = predict(train_X, train_y, lamda, threshold)\n        precision = np.argmax(train_predictions,axis=1) == np.argmax(train_y,axis=1)\n        epsilon = sum(boost[~precision]) \n        delta = epsilon / (1.0 - epsilon)\n        boost[precision] = boost[precision] * delta\n        boost = boost / sum(boost)\n        \n        \n        metrics[\"train_loss\"].append(results[\"train_loss\"][\"mean\"])\n        metrics[\"train_precision\"].append(results[\"train_precision\"][\"mean\"])\n        metrics[\"train_recall\"].append(results[\"train_recall\"][\"mean\"])\n        metrics[\"train_f\"].append(results[\"train_f\"][\"mean\"])\n        metrics[\"val_loss\"].append(results[\"val_loss\"][\"mean\"])\n        metrics[\"val_precision\"].append(results[\"val_precision\"][\"mean\"])\n        metrics[\"val_recall\"].append(results[\"val_recall\"][\"mean\"])\n        metrics[\"val_f\"].append(results[\"val_f\"][\"mean\"])\n        metrics[\"test_loss\"].append(results[\"test_loss\"][\"mean\"])\n        metrics[\"test_precision\"].append(results[\"test_precision\"][\"mean\"])\n        metrics[\"test_recall\"].append(results[\"test_recall\"][\"mean\"])\n        metrics[\"test_f\"].append(results[\"test_f\"][\"mean\"])\n        metrics[\"test_predictions\"].append(results[\"test_predictions\"])\n        \n\n\n    results = {\n        \"train_loss\": {\"mean\":np.nanmean(metrics[\"train_loss\"]), \"std\":np.nanstd(metrics[\"train_loss\"]), \"values\":metrics[\"train_loss\"]},\n        \"train_precision\": {\"mean\":np.nanmean(metrics[\"train_precision\"]), \"std\":np.nanstd(metrics[\"train_precision\"]), \"values\":metrics[\"train_precision\"]},\n        \"train_recall\": {\"mean\":np.nanmean(metrics[\"train_recall\"]), \"std\":np.nanstd(metrics[\"train_recall\"]), \"values\":metrics[\"train_recall\"]},\n        \"train_f\": {\"mean\":np.nanmean(metrics[\"train_f\"]), \"std\":np.nanstd(metrics[\"train_f\"]), \"values\":metrics[\"train_f\"]},\n        \"val_loss\": {\"mean\":np.nanmean(metrics[\"val_loss\"]), \"std\":np.nanstd(metrics[\"val_loss\"]), \"values\":metrics[\"val_loss\"]},\n        \"val_precision\":{\"mean\":np.nanmean(metrics[\"val_precision\"]), \"std\":np.nanstd(metrics[\"val_precision\"]), \"values\":metrics[\"val_precision\"]},\n        \"val_recall\": {\"mean\":np.nanmean(metrics[\"val_recall\"]), \"std\":np.nanstd(metrics[\"val_recall\"]), \"values\":metrics[\"val_recall\"]},\n        \"val_f\": {\"mean\":np.nanmean(metrics[\"val_f\"]), \"std\":np.nanstd(metrics[\"val_f\"]), \"values\":metrics[\"val_f\"]},\n        \"test_loss\": {\"mean\":np.nanmean(metrics[\"test_loss\"]), \"std\":np.nanstd(metrics[\"test_loss\"]), \"values\":metrics[\"test_loss\"]},\n        \"test_precision\":{\"mean\":np.nanmean(metrics[\"test_precision\"]), \"std\":np.nanstd(metrics[\"test_precision\"]), \"values\":metrics[\"test_precision\"]},\n        \"test_recall\": {\"mean\":np.nanmean(metrics[\"test_recall\"]), \"std\":np.nanstd(metrics[\"test_recall\"]), \"values\":metrics[\"test_recall\"]},\n        \"test_f\": {\"mean\":np.nanmean(metrics[\"test_f\"]), \"std\":np.nanstd(metrics[\"test_f\"]), \"values\":metrics[\"test_f\"]},\n        \"test_predictions\": metrics[\"test_predictions\"],\n        \"weights\":boost\n    }\n\n    if debug:\n        print(\"Iteration : %d Lambda : %.2f, Threshold : %.2f\" % (i, lamda, threshold))\n        print(\"Training loss : %.2f+/-%.2f, precision : %.2f+/-%.2f, recall : %.2f+/-%.2f, F : %.2f+/-%.2f\" % \n              (results[\"train_loss\"][\"mean\"], results[\"train_loss\"][\"std\"],\n               results[\"train_precision\"][\"mean\"], results[\"train_precision\"][\"std\"],\n               results[\"train_recall\"][\"mean\"], results[\"train_recall\"][\"std\"],\n               results[\"train_f\"][\"mean\"], results[\"train_f\"][\"std\"]))\n        print(\"Validation loss : %.2f+/-%.2f, precision : %.2f+/-%.2f, recall : %.2f+/-%.2f, F : %.2f+/-%.2f\" % \n              (results[\"val_loss\"][\"mean\"], results[\"val_loss\"][\"std\"],\n               results[\"val_precision\"][\"mean\"], results[\"val_precision\"][\"std\"],\n               results[\"val_recall\"][\"mean\"], results[\"val_recall\"][\"std\"],\n               results[\"val_f\"][\"mean\"], results[\"val_f\"][\"std\"]))\n        print(\"Test loss : %.2f+/-%.2f, precision : %.2f+/-%.2f, recall : %.2f+/-%.2f, F : %.2f+/-%.2f\" % \n              (results[\"test_loss\"][\"mean\"], results[\"test_loss\"][\"std\"],\n               results[\"test_precision\"][\"mean\"], results[\"test_precision\"][\"std\"],\n               results[\"test_recall\"][\"mean\"], results[\"test_recall\"][\"std\"],\n               results[\"test_f\"][\"mean\"], results[\"test_f\"][\"std\"]))\n\n    return results\n", 
            "cell_type": "code", 
            "execution_count": 9, 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "##\n## BOOTSTRAP/BOOSTING TRAINING WITH LOO\n##\n\nprint \"Training\",\npredictions = np.array([]).reshape(0,2)\nbstrapTrainingSet = training_set\nthreshold = .0\n_, test_y = split(test_set, NUM_FEATURES)\ninitialTestValue = 394\n\nbstrapTrainingSet = bstrapTrainingSet.append(pandas.DataFrame(test_set.values[:initialTestValue,:]))\n#print bstrapTrainingSet\n\ntry:\n    for i in range(initialTestValue,len(test_set),2):\n\n        test_rows = pandas.DataFrame(test_set.values[[i, i+1],:])\n        success = False\n        retry = 0\n        while ((~success) & (retry<5)):\n            try:\n                ## CHOOSE BOOTSTRAP OR BOOST\n                results = boostingTrain(bstrapTrainingSet, test_rows, .01, 20, False)\n                #results = bootstrapTrain(bstrapTrainingSet, test_rows, .01, 20, threshold, False)\n                predictions =  np.concatenate([predictions, np.nanmean(results[\"test_predictions\"], axis=0)])    \n                success = True\n            except ValueError:  \n                log.emit_log( {'app_name': 'Experiment2','type': 'error','message': \"ValueError - Retrying...\"})\n                retry = retry + 1\n                \n            \n        bstrapTrainingSet = bstrapTrainingSet.append(test_rows)\n        # Window\n        bstrapTrainingSet = bstrapTrainingSet[-len(training_set):]\n\n        res = evaluate(predictions, test_y[initialTestValue:initialTestValue+len(predictions),:], threshold)\n        msg = str(\"Results after %d iterations, %.2f precision, %.2f recall at %.2f threshold\" % (i+2, res[0], res[1], threshold))\n        print \".\"\n        print msg\n\n        log.emit_log( {'app_name': 'Experiment2','type': 'result','message': msg})\n        # 15/02/18 - Bluemix no longer using logmet for metrics\n        #metrics.emit_metric(name='Experiment2.precision', value=res[0])\n        #metrics.emit_metric(name='Experiment2.recall', value=res[1])\n\n        pandas.DataFrame(predictions).to_csv(\"results_new2.csv\", header=False, index=False)\n        put_file('Experiment2', \"results_new2.csv\")\n\n        # Try to free memory\n        gc.collect()\nexcept:\n    print(\"Unexpected error: %s\" % sys.exc_info()[0])\n    log.emit_log( {'app_name': 'Experiment2','type': 'error','message': str(\"Unexpected error: %s\" % sys.exc_info()[0])})\n    raise\n    ", 
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nResults after 396 iterations, 0.00 precision, 1.00 recall at 0.00 threshold\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nResults after 398 iterations, 0.50 precision, 1.00 recall at 0.00 threshold\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nResults after 400 iterations, 0.67 precision, 1.00 recall at 0.00 threshold\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nResults after 402 iterations, 0.75 precision, 1.00 recall at 0.00 threshold\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nResults after 404 iterations, 0.60 precision, 1.00 recall at 0.00 threshold\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nResults after 406 iterations, 0.67 precision, 1.00 recall at 0.00 threshold\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nResults after 408 iterations, 0.64 precision, 1.00 recall at 0.00 threshold\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nResults after 410 iterations, 0.56 precision, 1.00 recall at 0.00 threshold\n\n. . . . . . . . . . . . . . . . . . ."
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "##\n## BOOTSTRAP TRAINING\n##\n\nprint \"Training\",\n_, test_y = split(test_set, NUM_FEATURES)\nresults = bootstrapTrain(training_set, test_set, .1, 20, .0, True)\npredictions2 =  np.nanmean(results[\"test_predictions\"], axis=0)\nevaluate(predictions2, test_y, .5)", 
            "cell_type": "code", 
            "execution_count": 11, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Iteration : 19 Lambda : 0.10, Threshold : 0.00\nTraining loss : 0.61+/-0.02, precision : 0.66+/-0.02, recall : 1.00+/-0.00, F : 0.79+/-0.02\nValidation loss : 0.72+/-0.02, precision : 0.54+/-0.02, recall : 1.00+/-0.00, F : 0.70+/-0.01\nTest loss : 0.73+/-0.02, precision : 0.54+/-0.02, recall : 1.00+/-0.00, F : 0.70+/-0.02\n"
                }, 
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "(0.54883718, 1.0, 0.70870868836909162, array([[ 0.56349051,  0.43598217],\n        [ 0.55803794,  0.44130236],\n        [ 0.5493713 ,  0.45034656],\n        [ 0.55059552,  0.4489843 ],\n        [ 0.49326783,  0.50540805],\n        [ 0.45126668,  0.54793656],\n        [ 0.65567821,  0.34172443],\n        [ 0.60610175,  0.39114138],\n        [ 0.48215455,  0.51353896],\n        [ 0.46498594,  0.53102314],\n        [ 0.37359807,  0.62640458],\n        [ 0.42384404,  0.57626975],\n        [ 0.5093075 ,  0.49005389],\n        [ 0.55047226,  0.44859108],\n        [ 0.49225932,  0.50419605],\n        [ 0.51646715,  0.47996092],\n        [ 0.79734296,  0.20354548],\n        [ 0.62736219,  0.3730047 ],\n        [ 0.6468395 ,  0.35361773],\n        [ 0.62354386,  0.37692541],\n        [ 0.49336094,  0.50491095],\n        [ 0.56665027,  0.43155614],\n        [ 0.59447235,  0.40554476],\n        [ 0.68440908,  0.31626263],\n        [ 0.60287625,  0.39587271],\n        [ 0.35190681,  0.64677334],\n        [ 0.37363994,  0.62492323],\n        [ 0.53190124,  0.46834469],\n        [ 0.56040841,  0.43985391],\n        [ 0.4381001 ,  0.56178331],\n        [ 0.60314953,  0.39699331],\n        [ 0.44454059,  0.55529654],\n        [ 0.45496202,  0.54496253],\n        [ 0.48666853,  0.51060677],\n        [ 0.47532672,  0.52287948],\n        [ 0.59239781,  0.40752333],\n        [ 0.64675081,  0.35339308],\n        [ 0.7668643 ,  0.23319077],\n        [ 0.82283628,  0.17714676],\n        [ 0.69956267,  0.30022103],\n        [ 0.5798772 ,  0.4202489 ],\n        [ 0.66586792,  0.33432162],\n        [ 0.67764986,  0.32253858],\n        [ 0.60325444,  0.39663318],\n        [ 0.61292666,  0.38708395],\n        [ 0.54231304,  0.45781499],\n        [ 0.52552736,  0.47447667],\n        [ 0.68160552,  0.31895483],\n        [ 0.71362698,  0.28705853],\n        [ 0.48949033,  0.50713235],\n        [ 0.74978209,  0.24987483],\n        [ 0.79754418,  0.20258348],\n        [ 0.43083686,  0.5670982 ],\n        [ 0.60187954,  0.39558965],\n        [ 0.57497877,  0.42425117],\n        [ 0.55728018,  0.44131646],\n        [ 0.60466999,  0.39598161],\n        [ 0.46573567,  0.53489262],\n        [ 0.53068548,  0.46726584],\n        [ 0.45257378,  0.5459435 ],\n        [ 0.56866264,  0.43051353],\n        [ 0.57010752,  0.42899504],\n        [ 0.53537494,  0.46531922],\n        [ 0.44112229,  0.55937654],\n        [ 0.5673449 ,  0.43258128],\n        [ 0.56417328,  0.43558717],\n        [ 0.56051767,  0.43941212],\n        [ 0.49838576,  0.50132006],\n        [ 0.34718838,  0.65233493],\n        [ 0.34194043,  0.65741342],\n        [ 0.73271692,  0.26739058],\n        [ 0.67960304,  0.32063875],\n        [ 0.64777613,  0.35231584],\n        [ 0.63695782,  0.363159  ],\n        [ 0.66103965,  0.33871803],\n        [ 0.60738492,  0.39250535],\n        [ 0.67117423,  0.32904148],\n        [ 0.63778722,  0.36235639],\n        [ 0.39724785,  0.59751523],\n        [ 0.39023483,  0.60435647],\n        [ 0.50698173,  0.48972359],\n        [ 0.56628489,  0.43197599],\n        [ 0.63324535,  0.36594898],\n        [ 0.52857792,  0.47074217],\n        [ 0.45921469,  0.53715098],\n        [ 0.45755357,  0.5387013 ],\n        [ 0.37581277,  0.62354213],\n        [ 0.41259518,  0.58702308],\n        [ 0.63714272,  0.36244059],\n        [ 0.64189529,  0.35774589],\n        [ 0.47846174,  0.52280974],\n        [ 0.53273201,  0.46855631],\n        [ 0.42773151,  0.57190788],\n        [ 0.42210108,  0.57751387],\n        [ 0.61912483,  0.3811999 ],\n        [ 0.63745558,  0.36309895],\n        [ 0.62861979,  0.36975104],\n        [ 0.61473233,  0.38359779],\n        [ 0.46714824,  0.52941817],\n        [ 0.44168973,  0.55423933],\n        [ 0.49085122,  0.50916272],\n        [ 0.46857166,  0.53135878],\n        [ 0.50646245,  0.49115077],\n        [ 0.50488079,  0.49266329],\n        [ 0.81380051,  0.18705064],\n        [ 0.42665347,  0.57073259],\n        [ 0.44765848,  0.54927647],\n        [ 0.46124762,  0.53840697],\n        [ 0.44423485,  0.55513537],\n        [ 0.71555877,  0.28514582],\n        [ 0.74338877,  0.25711909],\n        [ 0.50101161,  0.49726361],\n        [ 0.50138748,  0.49716097],\n        [ 0.56440985,  0.4327454 ],\n        [ 0.60787189,  0.38994634],\n        [ 0.5824461 ,  0.41674528],\n        [ 0.53413385,  0.46560127],\n        [ 0.6661402 ,  0.3338972 ],\n        [ 0.6019336 ,  0.39793986],\n        [ 0.49942812,  0.50088519],\n        [ 0.40844679,  0.59151495],\n        [ 0.56537259,  0.43346101],\n        [ 0.37731823,  0.62243044],\n        [ 0.67996109,  0.31958026],\n        [ 0.75110209,  0.24957797],\n        [ 0.68939793,  0.31102341],\n        [ 0.61926889,  0.38098684],\n        [ 0.69114697,  0.30978927],\n        [ 0.65918428,  0.34167647],\n        [ 0.45189095,  0.54774123],\n        [ 0.47096148,  0.52877784],\n        [ 0.65242726,  0.34848207],\n        [ 0.68836153,  0.3124736 ],\n        [ 0.6171003 ,  0.38272029],\n        [ 0.61694771,  0.38289005],\n        [ 0.69897163,  0.30235535],\n        [ 0.68724823,  0.31399077],\n        [ 0.50221771,  0.49384624],\n        [ 0.5168072 ,  0.47866336],\n        [ 0.63553417,  0.36386368],\n        [ 0.54340369,  0.45637351],\n        [ 0.68333805,  0.31755176],\n        [ 0.71222746,  0.28898883],\n        [ 0.56859481,  0.42879978],\n        [ 0.50677067,  0.4916535 ],\n        [ 0.55916685,  0.44012523],\n        [ 0.49517098,  0.50457072],\n        [ 0.40525398,  0.59296477],\n        [ 0.54102826,  0.45851183],\n        [ 0.66937071,  0.33126751],\n        [ 0.60322779,  0.39756528],\n        [ 0.37594101,  0.62102532],\n        [ 0.35051581,  0.64721376],\n        [ 0.65089595,  0.3499079 ],\n        [ 0.74548292,  0.25519371],\n        [ 0.29915825,  0.69994092],\n        [ 0.28236932,  0.71579146],\n        [ 0.53741711,  0.46254927],\n        [ 0.5171597 ,  0.48251885],\n        [ 0.28924301,  0.71018058],\n        [ 0.3723557 ,  0.62733829],\n        [ 0.68839633,  0.31170946],\n        [ 0.55961359,  0.44025463],\n        [ 0.67035669,  0.32801095],\n        [ 0.67134011,  0.32792339],\n        [ 0.51318967,  0.48674124],\n        [ 0.53450531,  0.46583337],\n        [ 0.553774  ,  0.44622749],\n        [ 0.59329164,  0.40709901],\n        [ 0.53638518,  0.46362215],\n        [ 0.54413384,  0.45590544],\n        [ 0.46213251,  0.53587663],\n        [ 0.5259701 ,  0.47279748],\n        [ 0.59027147,  0.41034192],\n        [ 0.58115447,  0.41939536],\n        [ 0.58516896,  0.41589522],\n        [ 0.59319633,  0.40769738],\n        [ 0.34534478,  0.65272987],\n        [ 0.30583924,  0.69228184],\n        [ 0.51300251,  0.48543635],\n        [ 0.45342818,  0.54514062],\n        [ 0.53064507,  0.46757206],\n        [ 0.59732753,  0.40189236],\n        [ 0.45743114,  0.54240417],\n        [ 0.28466213,  0.71414739],\n        [ 0.63561761,  0.36518651],\n        [ 0.69407463,  0.30671138],\n        [ 0.71692765,  0.28318462],\n        [ 0.59187084,  0.40854368],\n        [ 0.54842472,  0.45007071],\n        [ 0.57539207,  0.42492366],\n        [ 0.49844417,  0.50013989],\n        [ 0.73753309,  0.263008  ],\n        [ 0.55233783,  0.4468399 ],\n        [ 0.65867102,  0.34162816],\n        [ 0.6506924 ,  0.3495093 ],\n        [ 0.75453031,  0.24519622],\n        [ 0.5301978 ,  0.47009364],\n        [ 0.542817  ,  0.45734635],\n        [ 0.44762579,  0.54876179],\n        [ 0.45095968,  0.54399371],\n        [ 0.51856393,  0.48066568],\n        [ 0.56638986,  0.43291169],\n        [ 0.60101444,  0.39915532],\n        [ 0.59046084,  0.40948582],\n        [ 0.65834463,  0.34006542],\n        [ 0.47755843,  0.52085072],\n        [ 0.68399704,  0.31692678],\n        [ 0.59190041,  0.40887338],\n        [ 0.52999079,  0.47047171],\n        [ 0.61097604,  0.38962007],\n        [ 0.48100781,  0.51722246],\n        [ 0.48527345,  0.51246226],\n        [ 0.55963475,  0.44042307],\n        [ 0.67621744,  0.32328016],\n        [ 0.56071705,  0.44066143],\n        [ 0.51329905,  0.4879787 ],\n        [ 0.28677934,  0.71392637],\n        [ 0.32119057,  0.67933083],\n        [ 0.54300493,  0.45702633],\n        [ 0.47317871,  0.52621847],\n        [ 0.51401263,  0.48322755],\n        [ 0.55361903,  0.44393414],\n        [ 0.53321326,  0.46759909],\n        [ 0.43433553,  0.56563747],\n        [ 0.58110094,  0.41953006],\n        [ 0.62935269,  0.37107977],\n        [ 0.55585128,  0.44040671],\n        [ 0.46825185,  0.52733898],\n        [ 0.70564669,  0.29448938],\n        [ 0.55846769,  0.44197845],\n        [ 0.51160252,  0.48831731],\n        [ 0.49882931,  0.50051618],\n        [ 0.60336143,  0.39708591],\n        [ 0.62013584,  0.38071519],\n        [ 0.53191686,  0.46784028],\n        [ 0.54242432,  0.45741791],\n        [ 0.47566819,  0.52350682],\n        [ 0.45090857,  0.54745305],\n        [ 0.57625467,  0.42412257],\n        [ 0.55223542,  0.44799748],\n        [ 0.49336511,  0.50205642],\n        [ 0.57640868,  0.41980043],\n        [ 0.46262449,  0.53606582],\n        [ 0.46401659,  0.53383571],\n        [ 0.63931841,  0.36022109],\n        [ 0.67162758,  0.32912153],\n        [ 0.7477479 ,  0.25253576],\n        [ 0.70947617,  0.29105359],\n        [ 0.59461957,  0.40527377],\n        [ 0.5320307 ,  0.46762285],\n        [ 0.45047134,  0.54891962],\n        [ 0.56197697,  0.43780333],\n        [ 0.62836778,  0.3710191 ],\n        [ 0.58045638,  0.41822115],\n        [ 0.3395223 ,  0.65909255],\n        [ 0.42990202,  0.56904668],\n        [ 0.73405492,  0.26605687],\n        [ 0.71590829,  0.28388944],\n        [ 0.50843394,  0.49197865],\n        [ 0.53421897,  0.46618944],\n        [ 0.60174572,  0.39682627],\n        [ 0.57906961,  0.41936746],\n        [ 0.54583734,  0.45447105],\n        [ 0.5632531 ,  0.43704548],\n        [ 0.6279465 ,  0.37267029],\n        [ 0.38592595,  0.61365449],\n        [ 0.35429388,  0.64516938],\n        [ 0.41458598,  0.58567423],\n        [ 0.42863148,  0.57157469],\n        [ 0.45013767,  0.55036271],\n        [ 0.48394832,  0.51669264],\n        [ 0.56436783,  0.4368422 ],\n        [ 0.47266045,  0.52602208],\n        [ 0.46860391,  0.52894545],\n        [ 0.60707378,  0.39149183],\n        [ 0.59307712,  0.40475386],\n        [ 0.54898185,  0.44736147],\n        [ 0.48801059,  0.50740576],\n        [ 0.47138205,  0.52407181],\n        [ 0.4776248 ,  0.51742852],\n        [ 0.64614981,  0.35433966],\n        [ 0.59690017,  0.40308747],\n        [ 0.47402558,  0.52553302],\n        [ 0.46071452,  0.53860378],\n        [ 0.56991363,  0.42707771],\n        [ 0.53913546,  0.45743698],\n        [ 0.61557364,  0.38360995],\n        [ 0.59147054,  0.40792933],\n        [ 0.49775761,  0.50251532],\n        [ 0.63279188,  0.36689806],\n        [ 0.44822326,  0.55209148],\n        [ 0.46606153,  0.5345149 ],\n        [ 0.54856437,  0.45115834],\n        [ 0.5070529 ,  0.49303871],\n        [ 0.69283044,  0.30841336],\n        [ 0.71607989,  0.28526625],\n        [ 0.75646168,  0.2442157 ],\n        [ 0.71959174,  0.28156179],\n        [ 0.63937026,  0.36145684],\n        [ 0.63245255,  0.36834517],\n        [ 0.5682807 ,  0.43111163],\n        [ 0.43718162,  0.56206703],\n        [ 0.60857391,  0.3889913 ],\n        [ 0.76567566,  0.23366162],\n        [ 0.50253761,  0.49463564],\n        [ 0.47956786,  0.518305  ],\n        [ 0.8368004 ,  0.16312653],\n        [ 0.6766504 ,  0.32401189],\n        [ 0.68783861,  0.31275064],\n        [ 0.47078162,  0.5277524 ],\n        [ 0.46351686,  0.5340749 ],\n        [ 0.44107738,  0.55813074],\n        [ 0.56720084,  0.43183333],\n        [ 0.48457879,  0.51531708],\n        [ 0.43796173,  0.56182837],\n        [ 0.52048004,  0.47990996],\n        [ 0.56762135,  0.43279251],\n        [ 0.53227109,  0.46455398],\n        [ 0.47314644,  0.52437031],\n        [ 0.59388053,  0.40465093],\n        [ 0.64293474,  0.35647774],\n        [ 0.61755437,  0.38147321],\n        [ 0.57410902,  0.42485422],\n        [ 0.57716298,  0.42372495],\n        [ 0.52798963,  0.47283712],\n        [ 0.56756872,  0.42973199],\n        [ 0.62743843,  0.36947539],\n        [ 0.59846377,  0.40225673],\n        [ 0.62421441,  0.37644666],\n        [ 0.51580733,  0.48415518],\n        [ 0.50904042,  0.49084902],\n        [ 0.59903371,  0.40007505],\n        [ 0.57609421,  0.42287144],\n        [ 0.51627362,  0.4823814 ],\n        [ 0.41574207,  0.5802151 ],\n        [ 0.73590815,  0.26459348],\n        [ 0.76467258,  0.23510173],\n        [ 0.57779306,  0.41997319],\n        [ 0.61197686,  0.38526338],\n        [ 0.51250088,  0.48747236],\n        [ 0.55625582,  0.44398522],\n        [ 0.66786587,  0.33307016],\n        [ 0.60188711,  0.3990151 ],\n        [ 0.3915222 ,  0.60787356],\n        [ 0.42131978,  0.57807767],\n        [ 0.48955497,  0.50881815],\n        [ 0.417317  ,  0.58010668],\n        [ 0.47766656,  0.5182597 ],\n        [ 0.45267239,  0.54282802],\n        [ 0.36089164,  0.63833088],\n        [ 0.4197982 ,  0.57922655],\n        [ 0.51565802,  0.48442894],\n        [ 0.50782299,  0.49223408],\n        [ 0.34139255,  0.65781707],\n        [ 0.33592883,  0.6623711 ],\n        [ 0.61873144,  0.38190421],\n        [ 0.63039374,  0.37026295],\n        [ 0.42209798,  0.57789314],\n        [ 0.510225  ,  0.48989087],\n        [ 0.6425522 ,  0.35818106],\n        [ 0.72399545,  0.27678156],\n        [ 0.60033917,  0.40014252],\n        [ 0.60085601,  0.39972079],\n        [ 0.48195568,  0.51490039],\n        [ 0.43205777,  0.56570244],\n        [ 0.58686733,  0.41352138],\n        [ 0.63424087,  0.36601844],\n        [ 0.53492922,  0.46549225],\n        [ 0.51198781,  0.48839736],\n        [ 0.60543698,  0.39576173],\n        [ 0.58524972,  0.41585541],\n        [ 0.69304281,  0.30810043],\n        [ 0.5473994 ,  0.45342809],\n        [ 0.53227872,  0.46791488],\n        [ 0.61773986,  0.38282233],\n        [ 0.64929283,  0.3506636 ],\n        [ 0.62864184,  0.37057692],\n        [ 0.45413595,  0.54608762],\n        [ 0.4605813 ,  0.53961837],\n        [ 0.47020221,  0.52512008],\n        [ 0.50508893,  0.4909336 ],\n        [ 0.69525778,  0.30521259],\n        [ 0.69911999,  0.30164301],\n        [ 0.60838401,  0.39187747],\n        [ 0.61214441,  0.38804823],\n        [ 0.4357006 ,  0.5640384 ],\n        [ 0.4056274 ,  0.59398508],\n        [ 0.5428229 ,  0.45782328],\n        [ 0.55147147,  0.44888002],\n        [ 0.6562137 ,  0.34330767],\n        [ 0.61180723,  0.38757047],\n        [ 0.56403804,  0.43675485],\n        [ 0.61162651,  0.38872641],\n        [ 0.69127953,  0.30937091],\n        [ 0.6508497 ,  0.35000309],\n        [ 0.4551689 ,  0.54454869],\n        [ 0.45332795,  0.5460009 ],\n        [ 0.43084654,  0.56721544],\n        [ 0.43884   ,  0.55989152],\n        [ 0.60193408,  0.39772898],\n        [ 0.55028683,  0.44942427],\n        [ 0.56211305,  0.43820673],\n        [ 0.50268573,  0.49765295],\n        [ 0.69801801,  0.30245262],\n        [ 0.54645073,  0.45368189],\n        [ 0.50802934,  0.49012899],\n        [ 0.43766189,  0.55954754],\n        [ 0.72981346,  0.27060255],\n        [ 0.76045954,  0.23973426],\n        [ 0.61629266,  0.38378358],\n        [ 0.58619386,  0.41338927],\n        [ 0.68384135,  0.31656164],\n        [ 0.484438  ,  0.51561987],\n        [ 0.73265588,  0.26680726],\n        [ 0.63487005,  0.36439055],\n        [ 0.60640943,  0.39394444],\n        [ 0.74921447,  0.25078014],\n        [ 0.560085  ,  0.44054455],\n        [ 0.54194057,  0.45859247],\n        [ 0.61631167,  0.38297975],\n        [ 0.51066601,  0.48889661],\n        [ 0.63045335,  0.37049347],\n        [ 0.58907712,  0.41157594],\n        [ 0.50552243,  0.49398232],\n        [ 0.51411867,  0.48537365],\n        [ 0.39050871,  0.60927045],\n        [ 0.38463581,  0.61517668],\n        [ 0.57231343,  0.4276436 ],\n        [ 0.52495223,  0.47443604]], dtype=float32))"
                    }, 
                    "execution_count": 11
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }
    ], 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 2.0", 
            "name": "python2-spark20", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "2.7.11", 
            "name": "python", 
            "pygments_lexer": "ipython2", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }
        }
    }
}